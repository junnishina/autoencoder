{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "もしパーセプトロン1個を1ビットと考えれば、究極的には中間層が1個あれば、４と９を分離するには十分なはずである。この考え方は正しいのだろうか。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 200\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_train_idx = np.logical_or(y_train == 4, y_train == 9)\n",
    "keep_test_idx = np.logical_or(y_test ==4, y_test == 9)\n",
    "\n",
    "x_train = x_train[keep_train_idx]\n",
    "x_test = x_test[keep_test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_vec = x_train.reshape(x_train.shape[0], 784)\n",
    "x_test_vec = x_test.reshape(x_test.shape[0], 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 1)                 785       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 784)               1568      \n",
      "=================================================================\n",
      "Total params: 2,353\n",
      "Trainable params: 2,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1, activation='sigmoid', input_shape=(784,)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(784, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11791 samples, validate on 1991 samples\n",
      "Epoch 1/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.2117 - acc: 0.0058 - val_loss: 0.1944 - val_acc: 0.0060\n",
      "Epoch 2/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.1804 - acc: 0.0058 - val_loss: 0.1654 - val_acc: 0.0060\n",
      "Epoch 3/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.1548 - acc: 0.0076 - val_loss: 0.1409 - val_acc: 0.0060\n",
      "Epoch 4/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.1334 - acc: 0.0062 - val_loss: 0.1207 - val_acc: 0.0060\n",
      "Epoch 5/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.1171 - acc: 0.0059 - val_loss: 0.1043 - val_acc: 0.0060\n",
      "Epoch 6/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.1039 - acc: 0.0072 - val_loss: 0.0913 - val_acc: 0.0060\n",
      "Epoch 7/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0939 - acc: 0.0075 - val_loss: 0.0812 - val_acc: 0.0095\n",
      "Epoch 8/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0858 - acc: 0.0103 - val_loss: 0.0735 - val_acc: 0.0095\n",
      "Epoch 9/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0793 - acc: 0.0103 - val_loss: 0.0676 - val_acc: 0.0095\n",
      "Epoch 10/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0742 - acc: 0.0103 - val_loss: 0.0633 - val_acc: 0.0095\n",
      "Epoch 11/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0707 - acc: 0.0103 - val_loss: 0.0601 - val_acc: 0.0095\n",
      "Epoch 12/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0672 - acc: 0.0103 - val_loss: 0.0578 - val_acc: 0.0095\n",
      "Epoch 13/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0651 - acc: 0.0103 - val_loss: 0.0561 - val_acc: 0.0095\n",
      "Epoch 14/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0630 - acc: 0.0103 - val_loss: 0.0549 - val_acc: 0.0095\n",
      "Epoch 15/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0614 - acc: 0.0103 - val_loss: 0.0541 - val_acc: 0.0095\n",
      "Epoch 16/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0600 - acc: 0.0103 - val_loss: 0.0535 - val_acc: 0.0095\n",
      "Epoch 17/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0590 - acc: 0.0103 - val_loss: 0.0530 - val_acc: 0.0095\n",
      "Epoch 18/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0580 - acc: 0.0103 - val_loss: 0.0527 - val_acc: 0.0095\n",
      "Epoch 19/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0571 - acc: 0.0103 - val_loss: 0.0525 - val_acc: 0.0095\n",
      "Epoch 20/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0565 - acc: 0.0103 - val_loss: 0.0524 - val_acc: 0.0095\n",
      "Epoch 21/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0558 - acc: 0.0103 - val_loss: 0.0523 - val_acc: 0.0095\n",
      "Epoch 22/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0554 - acc: 0.0103 - val_loss: 0.0522 - val_acc: 0.0095\n",
      "Epoch 23/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0549 - acc: 0.0103 - val_loss: 0.0521 - val_acc: 0.0095\n",
      "Epoch 24/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0546 - acc: 0.0103 - val_loss: 0.0521 - val_acc: 0.0095\n",
      "Epoch 25/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0543 - acc: 0.0103 - val_loss: 0.0521 - val_acc: 0.0095\n",
      "Epoch 26/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0540 - acc: 0.0103 - val_loss: 0.0521 - val_acc: 0.0095\n",
      "Epoch 27/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0538 - acc: 0.0103 - val_loss: 0.0521 - val_acc: 0.0095\n",
      "Epoch 28/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0536 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 29/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0535 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 30/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0533 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 31/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0532 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 32/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0531 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 33/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0530 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 34/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0530 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 35/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0529 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 36/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0529 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 37/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0528 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 38/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0528 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 39/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0528 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 40/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0527 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 41/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0527 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 42/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0527 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 43/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0527 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 44/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0527 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 45/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0527 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 46/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0527 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 47/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0527 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 48/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0527 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 49/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0527 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 50/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 51/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 52/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 53/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 54/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 55/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 56/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 57/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 58/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 59/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 60/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 61/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 62/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 63/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 64/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 65/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 66/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 67/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 68/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 69/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 70/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 71/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 72/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 73/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 74/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 75/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 76/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 77/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 78/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 79/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 80/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 81/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 82/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 83/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 84/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 85/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 86/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 87/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 88/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 89/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 90/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 91/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 92/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 93/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 94/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 95/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 96/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 97/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 98/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 99/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 100/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 101/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 102/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 103/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 104/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 105/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 106/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 107/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 108/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 109/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 110/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 111/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 112/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 113/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 114/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 115/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 116/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 117/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 118/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 119/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 120/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 121/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 122/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 123/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 124/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 125/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 126/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 127/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 128/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 129/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 130/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 131/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 132/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 133/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 134/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 135/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 136/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 137/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 138/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 139/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 140/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 141/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 142/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 143/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 144/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 145/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 146/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 147/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 148/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 149/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 150/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 151/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 152/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 153/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 154/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 155/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 156/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 157/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 158/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 159/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 160/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 161/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 162/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 163/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 164/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 165/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 166/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 167/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 168/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 169/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 170/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 171/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 172/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 173/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 174/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 175/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 176/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 177/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 178/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 179/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 180/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 181/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 182/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 183/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 184/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 185/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 186/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 187/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 188/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 189/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 190/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 191/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 192/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 193/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 194/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 195/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 196/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 197/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 198/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 199/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 200/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Test loss: 0.0520159880729\n",
      "Test accuracy: 0.0095429432446\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train_vec, x_train_vec,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test_vec, x_test_vec))\n",
    "score = model.evaluate(x_test_vec, x_test_vec, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この場合、70エポックぐらいですでに改善が止まってしまっている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHEAAADqCAYAAAAlBtnSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDJZ3z3KM2AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24600a9af60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "n = 10  # how many digits we will display\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_train[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    predicted = model.predict(x_train_vec[i:i+1]).reshape((28,28))\n",
    "    plt.imshow(predicted)\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "４と９を分離してくれなくなってしまった。入力をほとんど同一の画像に収束してしまっており、さっきとは別の失敗モードである。とはいえ、４または９の大局的な形がこんなもんだという事については、把握してくれたようだ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
