{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "もしパーセプトロン1個を1ビットと考えれば、究極的には中間層が1個あれば、４と９を分離するには十分なはずである。この考え方は正しいのだろうか。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 200\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_train_idx = np.logical_or(y_train == 4, y_train == 9)\n",
    "keep_test_idx = np.logical_or(y_test ==4, y_test == 9)\n",
    "\n",
    "x_train = x_train[keep_train_idx]\n",
    "x_test = x_test[keep_test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_vec = x_train.reshape(x_train.shape[0], 784)\n",
    "x_test_vec = x_test.reshape(x_test.shape[0], 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1)                 785       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 784)               1568      \n",
      "=================================================================\n",
      "Total params: 2,353\n",
      "Trainable params: 2,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1, activation='sigmoid', input_shape=(784,)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(784, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11791 samples, validate on 1991 samples\n",
      "Epoch 1/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.2112 - acc: 0.0069 - val_loss: 0.1937 - val_acc: 0.0090\n",
      "Epoch 2/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.1801 - acc: 0.0076 - val_loss: 0.1648 - val_acc: 0.0090\n",
      "Epoch 3/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.1542 - acc: 0.0084 - val_loss: 0.1404 - val_acc: 0.0090\n",
      "Epoch 4/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.1331 - acc: 0.0076 - val_loss: 0.1203 - val_acc: 0.0090\n",
      "Epoch 5/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.1167 - acc: 0.0070 - val_loss: 0.1040 - val_acc: 0.0090\n",
      "Epoch 6/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.1031 - acc: 0.0100 - val_loss: 0.0911 - val_acc: 0.0095\n",
      "Epoch 7/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0934 - acc: 0.0103 - val_loss: 0.0810 - val_acc: 0.0095\n",
      "Epoch 8/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0853 - acc: 0.0103 - val_loss: 0.0733 - val_acc: 0.0095\n",
      "Epoch 9/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0793 - acc: 0.0103 - val_loss: 0.0675 - val_acc: 0.0095\n",
      "Epoch 10/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0746 - acc: 0.0103 - val_loss: 0.0632 - val_acc: 0.0095\n",
      "Epoch 11/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0708 - acc: 0.0103 - val_loss: 0.0601 - val_acc: 0.0095\n",
      "Epoch 12/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0677 - acc: 0.0103 - val_loss: 0.0577 - val_acc: 0.0095\n",
      "Epoch 13/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0648 - acc: 0.0103 - val_loss: 0.0561 - val_acc: 0.0095\n",
      "Epoch 14/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0632 - acc: 0.0103 - val_loss: 0.0549 - val_acc: 0.0095\n",
      "Epoch 15/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0614 - acc: 0.0103 - val_loss: 0.0540 - val_acc: 0.0095\n",
      "Epoch 16/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0600 - acc: 0.0103 - val_loss: 0.0534 - val_acc: 0.0095\n",
      "Epoch 17/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0589 - acc: 0.0103 - val_loss: 0.0530 - val_acc: 0.0095\n",
      "Epoch 18/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0580 - acc: 0.0103 - val_loss: 0.0527 - val_acc: 0.0095\n",
      "Epoch 19/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0571 - acc: 0.0103 - val_loss: 0.0525 - val_acc: 0.0095\n",
      "Epoch 20/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0565 - acc: 0.0103 - val_loss: 0.0524 - val_acc: 0.0095\n",
      "Epoch 21/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0559 - acc: 0.0103 - val_loss: 0.0523 - val_acc: 0.0095\n",
      "Epoch 22/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0554 - acc: 0.0103 - val_loss: 0.0522 - val_acc: 0.0095\n",
      "Epoch 23/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0550 - acc: 0.0103 - val_loss: 0.0522 - val_acc: 0.0095\n",
      "Epoch 24/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0546 - acc: 0.0103 - val_loss: 0.0521 - val_acc: 0.0095\n",
      "Epoch 25/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0543 - acc: 0.0103 - val_loss: 0.0521 - val_acc: 0.0095\n",
      "Epoch 26/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0540 - acc: 0.0103 - val_loss: 0.0521 - val_acc: 0.0095\n",
      "Epoch 27/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0538 - acc: 0.0103 - val_loss: 0.0521 - val_acc: 0.0095\n",
      "Epoch 28/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0536 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 29/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0534 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 30/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0533 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 31/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0532 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 32/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0531 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 33/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0530 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 34/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0530 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 35/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0529 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 36/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0529 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 37/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0528 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 38/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0528 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 39/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0528 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 40/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0527 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 41/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0527 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 42/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0527 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 43/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0527 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 44/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0527 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 45/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0527 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 46/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0527 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 47/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0527 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 48/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0527 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 49/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0527 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 50/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 51/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 52/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 53/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 54/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 55/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 56/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 57/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 58/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 59/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 60/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 61/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 62/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 63/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 64/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 65/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 66/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 67/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 68/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 69/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 70/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 71/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 72/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 73/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 74/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 75/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 76/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 77/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 78/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 79/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 80/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 81/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 82/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 83/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 84/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 85/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 86/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 87/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 88/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 89/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 90/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 91/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 92/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 93/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 94/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 95/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 96/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 97/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 98/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 99/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 100/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 101/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 102/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 103/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 104/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 105/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 106/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 107/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 108/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 109/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 110/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 111/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 112/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 113/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 114/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 115/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 116/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 117/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 118/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 119/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 120/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 121/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 122/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 123/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 124/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 125/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 126/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 127/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 128/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 129/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 130/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 131/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 132/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 133/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 134/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 135/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 136/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 137/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 138/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 139/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 140/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 141/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 142/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 143/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 144/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 145/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 146/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 147/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 148/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 149/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 150/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 151/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 152/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 153/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 154/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 155/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 156/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 157/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 158/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 159/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 160/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 161/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 162/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 163/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 164/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 165/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 166/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 167/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 168/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 169/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 170/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 171/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 172/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 173/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 174/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 175/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 176/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 177/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 178/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 179/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 180/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 181/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 182/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 183/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 184/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 185/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 186/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 187/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 188/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 189/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 190/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 191/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 192/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 193/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 194/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 195/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 196/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 197/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 198/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 199/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 200/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0526 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Test loss: 0.0520216984322\n",
      "Test accuracy: 0.0095429432446\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train_vec, x_train_vec,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test_vec, x_test_vec))\n",
    "score = model.evaluate(x_test_vec, x_test_vec, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この場合、70エポックぐらいですでに改善が止まってしまっている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHEAAADqCAYAAAAlBtnSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17c4f5a39e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "n = 10  # how many digits we will display\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_train[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    predicted = model.predict(x_train_vec[i:i+1]).reshape((28,28))\n",
    "    plt.imshow(predicted)\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "４と９を分離してくれなくなってしまった。入力をほとんど同一の画像に収束してしまっており、さっきとは別の失敗モードである。とはいえ、４または９の大局的な形がこんなもんだという事については、把握してくれたようだ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
