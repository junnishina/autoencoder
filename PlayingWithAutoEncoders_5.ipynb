{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多層のdeepencoderを試みる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 300\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_train_idx = np.logical_or(y_train == 4, y_train == 9)\n",
    "keep_test_idx = np.logical_or(y_test ==4, y_test == 9)\n",
    "\n",
    "x_train = x_train[keep_train_idx]\n",
    "x_test = x_test[keep_test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_vec = x_train.reshape(x_train.shape[0], 784)\n",
    "x_test_vec = x_test.reshape(x_test.shape[0], 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中央の要素数は先ほどと同じ12個に設定する。層間の比をだいたい同じにすると良い。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 12)                1212      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               1300      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 784)               79184     \n",
      "=================================================================\n",
      "Total params: 160,196\n",
      "Trainable params: 160,196\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, activation='sigmoid', input_shape=(784,)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(12, activation='sigmoid'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(100, activation='sigmoid'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(784, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11791 samples, validate on 1991 samples\n",
      "Epoch 1/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0747 - acc: 0.0066 - val_loss: 0.0524 - val_acc: 0.0095\n",
      "Epoch 2/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0531 - acc: 0.0081 - val_loss: 0.0517 - val_acc: 0.0095\n",
      "Epoch 3/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0515 - acc: 0.0072 - val_loss: 0.0489 - val_acc: 0.0080\n",
      "Epoch 4/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0483 - acc: 0.0093 - val_loss: 0.0460 - val_acc: 0.0105\n",
      "Epoch 5/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0467 - acc: 0.0107 - val_loss: 0.0444 - val_acc: 0.0065\n",
      "Epoch 6/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0452 - acc: 0.0109 - val_loss: 0.0423 - val_acc: 0.0080\n",
      "Epoch 7/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0437 - acc: 0.0093 - val_loss: 0.0408 - val_acc: 0.0070\n",
      "Epoch 8/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0429 - acc: 0.0097 - val_loss: 0.0404 - val_acc: 0.0085\n",
      "Epoch 9/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0425 - acc: 0.0104 - val_loss: 0.0400 - val_acc: 0.0110\n",
      "Epoch 10/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0422 - acc: 0.0106 - val_loss: 0.0399 - val_acc: 0.0131\n",
      "Epoch 11/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0421 - acc: 0.0101 - val_loss: 0.0396 - val_acc: 0.0116\n",
      "Epoch 12/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0418 - acc: 0.0100 - val_loss: 0.0394 - val_acc: 0.0121\n",
      "Epoch 13/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0417 - acc: 0.0094 - val_loss: 0.0391 - val_acc: 0.0116\n",
      "Epoch 14/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0415 - acc: 0.0102 - val_loss: 0.0390 - val_acc: 0.0121\n",
      "Epoch 15/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0414 - acc: 0.0100 - val_loss: 0.0387 - val_acc: 0.0126\n",
      "Epoch 16/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0412 - acc: 0.0098 - val_loss: 0.0386 - val_acc: 0.0105\n",
      "Epoch 17/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0409 - acc: 0.0124 - val_loss: 0.0382 - val_acc: 0.0100\n",
      "Epoch 18/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0407 - acc: 0.0120 - val_loss: 0.0381 - val_acc: 0.0146\n",
      "Epoch 19/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0405 - acc: 0.0127 - val_loss: 0.0376 - val_acc: 0.0131\n",
      "Epoch 20/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0402 - acc: 0.0092 - val_loss: 0.0374 - val_acc: 0.0136\n",
      "Epoch 21/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0400 - acc: 0.0109 - val_loss: 0.0372 - val_acc: 0.0131\n",
      "Epoch 22/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0399 - acc: 0.0114 - val_loss: 0.0368 - val_acc: 0.0136\n",
      "Epoch 23/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0398 - acc: 0.0109 - val_loss: 0.0367 - val_acc: 0.0121\n",
      "Epoch 24/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0396 - acc: 0.0089 - val_loss: 0.0364 - val_acc: 0.0105\n",
      "Epoch 25/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0395 - acc: 0.0102 - val_loss: 0.0363 - val_acc: 0.0121\n",
      "Epoch 26/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0393 - acc: 0.0097 - val_loss: 0.0361 - val_acc: 0.0131\n",
      "Epoch 27/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0391 - acc: 0.0096 - val_loss: 0.0359 - val_acc: 0.0131\n",
      "Epoch 28/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0390 - acc: 0.0107 - val_loss: 0.0358 - val_acc: 0.0151\n",
      "Epoch 29/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0390 - acc: 0.0096 - val_loss: 0.0356 - val_acc: 0.0100\n",
      "Epoch 30/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0389 - acc: 0.0103 - val_loss: 0.0356 - val_acc: 0.0141\n",
      "Epoch 31/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0387 - acc: 0.0092 - val_loss: 0.0354 - val_acc: 0.0126\n",
      "Epoch 32/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0386 - acc: 0.0084 - val_loss: 0.0354 - val_acc: 0.0100\n",
      "Epoch 33/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0384 - acc: 0.0087 - val_loss: 0.0351 - val_acc: 0.0131\n",
      "Epoch 34/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0384 - acc: 0.0097 - val_loss: 0.0350 - val_acc: 0.0136\n",
      "Epoch 35/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0383 - acc: 0.0108 - val_loss: 0.0348 - val_acc: 0.0121\n",
      "Epoch 36/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0383 - acc: 0.0080 - val_loss: 0.0349 - val_acc: 0.0126\n",
      "Epoch 37/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0381 - acc: 0.0079 - val_loss: 0.0348 - val_acc: 0.0110\n",
      "Epoch 38/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0381 - acc: 0.0092 - val_loss: 0.0345 - val_acc: 0.0100\n",
      "Epoch 39/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0380 - acc: 0.0100 - val_loss: 0.0344 - val_acc: 0.0116\n",
      "Epoch 40/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0378 - acc: 0.0089 - val_loss: 0.0343 - val_acc: 0.0090\n",
      "Epoch 41/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0378 - acc: 0.0098 - val_loss: 0.0342 - val_acc: 0.0085\n",
      "Epoch 42/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0377 - acc: 0.0089 - val_loss: 0.0341 - val_acc: 0.0080\n",
      "Epoch 43/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0376 - acc: 0.0088 - val_loss: 0.0340 - val_acc: 0.0090\n",
      "Epoch 44/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0375 - acc: 0.0087 - val_loss: 0.0338 - val_acc: 0.0075\n",
      "Epoch 45/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0374 - acc: 0.0080 - val_loss: 0.0338 - val_acc: 0.0075\n",
      "Epoch 46/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0373 - acc: 0.0084 - val_loss: 0.0336 - val_acc: 0.0095\n",
      "Epoch 47/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0373 - acc: 0.0094 - val_loss: 0.0335 - val_acc: 0.0055\n",
      "Epoch 48/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0373 - acc: 0.0109 - val_loss: 0.0334 - val_acc: 0.0075\n",
      "Epoch 49/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0372 - acc: 0.0087 - val_loss: 0.0333 - val_acc: 0.0080\n",
      "Epoch 50/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0371 - acc: 0.0098 - val_loss: 0.0332 - val_acc: 0.0095\n",
      "Epoch 51/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0369 - acc: 0.0099 - val_loss: 0.0331 - val_acc: 0.0090\n",
      "Epoch 52/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0369 - acc: 0.0103 - val_loss: 0.0329 - val_acc: 0.0080\n",
      "Epoch 53/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0368 - acc: 0.0098 - val_loss: 0.0328 - val_acc: 0.0080\n",
      "Epoch 54/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0368 - acc: 0.0101 - val_loss: 0.0327 - val_acc: 0.0090\n",
      "Epoch 55/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0367 - acc: 0.0109 - val_loss: 0.0325 - val_acc: 0.0105\n",
      "Epoch 56/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0366 - acc: 0.0106 - val_loss: 0.0324 - val_acc: 0.0100\n",
      "Epoch 57/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0366 - acc: 0.0103 - val_loss: 0.0324 - val_acc: 0.0080\n",
      "Epoch 58/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0364 - acc: 0.0114 - val_loss: 0.0323 - val_acc: 0.0116\n",
      "Epoch 59/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0364 - acc: 0.0110 - val_loss: 0.0322 - val_acc: 0.0131\n",
      "Epoch 60/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0363 - acc: 0.0109 - val_loss: 0.0321 - val_acc: 0.0090\n",
      "Epoch 61/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0363 - acc: 0.0096 - val_loss: 0.0320 - val_acc: 0.0090\n",
      "Epoch 62/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0363 - acc: 0.0112 - val_loss: 0.0319 - val_acc: 0.0116\n",
      "Epoch 63/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0362 - acc: 0.0097 - val_loss: 0.0318 - val_acc: 0.0105\n",
      "Epoch 64/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0362 - acc: 0.0124 - val_loss: 0.0317 - val_acc: 0.0095\n",
      "Epoch 65/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0361 - acc: 0.0107 - val_loss: 0.0316 - val_acc: 0.0105\n",
      "Epoch 66/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0361 - acc: 0.0090 - val_loss: 0.0316 - val_acc: 0.0090\n",
      "Epoch 67/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0360 - acc: 0.0094 - val_loss: 0.0315 - val_acc: 0.0095\n",
      "Epoch 68/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0360 - acc: 0.0118 - val_loss: 0.0315 - val_acc: 0.0116\n",
      "Epoch 69/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0360 - acc: 0.0097 - val_loss: 0.0314 - val_acc: 0.0095\n",
      "Epoch 70/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0359 - acc: 0.0090 - val_loss: 0.0314 - val_acc: 0.0110\n",
      "Epoch 71/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0360 - acc: 0.0094 - val_loss: 0.0313 - val_acc: 0.0116\n",
      "Epoch 72/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0358 - acc: 0.0090 - val_loss: 0.0313 - val_acc: 0.0105\n",
      "Epoch 73/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0358 - acc: 0.0096 - val_loss: 0.0311 - val_acc: 0.0095\n",
      "Epoch 74/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0358 - acc: 0.0109 - val_loss: 0.0312 - val_acc: 0.0095\n",
      "Epoch 75/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0357 - acc: 0.0087 - val_loss: 0.0310 - val_acc: 0.0110\n",
      "Epoch 76/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0357 - acc: 0.0110 - val_loss: 0.0310 - val_acc: 0.0080\n",
      "Epoch 77/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0357 - acc: 0.0093 - val_loss: 0.0309 - val_acc: 0.0105\n",
      "Epoch 78/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0355 - acc: 0.0109 - val_loss: 0.0309 - val_acc: 0.0095\n",
      "Epoch 79/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0357 - acc: 0.0110 - val_loss: 0.0308 - val_acc: 0.0121\n",
      "Epoch 80/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0356 - acc: 0.0093 - val_loss: 0.0308 - val_acc: 0.0100\n",
      "Epoch 81/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0355 - acc: 0.0098 - val_loss: 0.0307 - val_acc: 0.0085\n",
      "Epoch 82/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0354 - acc: 0.0092 - val_loss: 0.0306 - val_acc: 0.0105\n",
      "Epoch 83/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0355 - acc: 0.0100 - val_loss: 0.0306 - val_acc: 0.0095\n",
      "Epoch 84/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0354 - acc: 0.0107 - val_loss: 0.0306 - val_acc: 0.0126\n",
      "Epoch 85/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0354 - acc: 0.0108 - val_loss: 0.0305 - val_acc: 0.0095\n",
      "Epoch 86/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0353 - acc: 0.0099 - val_loss: 0.0304 - val_acc: 0.0090\n",
      "Epoch 87/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0353 - acc: 0.0093 - val_loss: 0.0305 - val_acc: 0.0090\n",
      "Epoch 88/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0353 - acc: 0.0089 - val_loss: 0.0305 - val_acc: 0.0100\n",
      "Epoch 89/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0352 - acc: 0.0109 - val_loss: 0.0304 - val_acc: 0.0095\n",
      "Epoch 90/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0352 - acc: 0.0102 - val_loss: 0.0304 - val_acc: 0.0100\n",
      "Epoch 91/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0353 - acc: 0.0099 - val_loss: 0.0304 - val_acc: 0.0080\n",
      "Epoch 92/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0351 - acc: 0.0085 - val_loss: 0.0303 - val_acc: 0.0100\n",
      "Epoch 93/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0352 - acc: 0.0110 - val_loss: 0.0302 - val_acc: 0.0085\n",
      "Epoch 94/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0351 - acc: 0.0097 - val_loss: 0.0302 - val_acc: 0.0105\n",
      "Epoch 95/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0351 - acc: 0.0103 - val_loss: 0.0302 - val_acc: 0.0100\n",
      "Epoch 96/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0352 - acc: 0.0094 - val_loss: 0.0302 - val_acc: 0.0085\n",
      "Epoch 97/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0350 - acc: 0.0098 - val_loss: 0.0301 - val_acc: 0.0090\n",
      "Epoch 98/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0351 - acc: 0.0093 - val_loss: 0.0301 - val_acc: 0.0085\n",
      "Epoch 99/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0349 - acc: 0.0090 - val_loss: 0.0300 - val_acc: 0.0095\n",
      "Epoch 100/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0351 - acc: 0.0102 - val_loss: 0.0300 - val_acc: 0.0100\n",
      "Epoch 101/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0350 - acc: 0.0087 - val_loss: 0.0299 - val_acc: 0.0095\n",
      "Epoch 102/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0350 - acc: 0.0115 - val_loss: 0.0299 - val_acc: 0.0095\n",
      "Epoch 103/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0349 - acc: 0.0108 - val_loss: 0.0299 - val_acc: 0.0100\n",
      "Epoch 104/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0349 - acc: 0.0102 - val_loss: 0.0299 - val_acc: 0.0095\n",
      "Epoch 105/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0349 - acc: 0.0103 - val_loss: 0.0298 - val_acc: 0.0085\n",
      "Epoch 106/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0349 - acc: 0.0103 - val_loss: 0.0298 - val_acc: 0.0095\n",
      "Epoch 107/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0348 - acc: 0.0112 - val_loss: 0.0298 - val_acc: 0.0080\n",
      "Epoch 108/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0347 - acc: 0.0096 - val_loss: 0.0297 - val_acc: 0.0095\n",
      "Epoch 109/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0348 - acc: 0.0100 - val_loss: 0.0297 - val_acc: 0.0075\n",
      "Epoch 110/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0348 - acc: 0.0089 - val_loss: 0.0297 - val_acc: 0.0095\n",
      "Epoch 111/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0348 - acc: 0.0093 - val_loss: 0.0297 - val_acc: 0.0100\n",
      "Epoch 112/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0348 - acc: 0.0092 - val_loss: 0.0296 - val_acc: 0.0080\n",
      "Epoch 113/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0347 - acc: 0.0102 - val_loss: 0.0296 - val_acc: 0.0110\n",
      "Epoch 114/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0346 - acc: 0.0097 - val_loss: 0.0295 - val_acc: 0.0075\n",
      "Epoch 115/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0347 - acc: 0.0096 - val_loss: 0.0296 - val_acc: 0.0090\n",
      "Epoch 116/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0347 - acc: 0.0106 - val_loss: 0.0295 - val_acc: 0.0070\n",
      "Epoch 117/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0347 - acc: 0.0091 - val_loss: 0.0294 - val_acc: 0.0075\n",
      "Epoch 118/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0347 - acc: 0.0084 - val_loss: 0.0295 - val_acc: 0.0080\n",
      "Epoch 119/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0347 - acc: 0.0103 - val_loss: 0.0294 - val_acc: 0.0065\n",
      "Epoch 120/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0345 - acc: 0.0115 - val_loss: 0.0293 - val_acc: 0.0095\n",
      "Epoch 121/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0346 - acc: 0.0102 - val_loss: 0.0293 - val_acc: 0.0075\n",
      "Epoch 122/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0345 - acc: 0.0089 - val_loss: 0.0293 - val_acc: 0.0095\n",
      "Epoch 123/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0346 - acc: 0.0098 - val_loss: 0.0293 - val_acc: 0.0105\n",
      "Epoch 124/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0345 - acc: 0.0096 - val_loss: 0.0293 - val_acc: 0.0100\n",
      "Epoch 125/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0344 - acc: 0.0103 - val_loss: 0.0292 - val_acc: 0.0090\n",
      "Epoch 126/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0344 - acc: 0.0103 - val_loss: 0.0292 - val_acc: 0.0126\n",
      "Epoch 127/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11791/11791 [==============================] - 1s - loss: 0.0345 - acc: 0.0094 - val_loss: 0.0291 - val_acc: 0.0085\n",
      "Epoch 128/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0346 - acc: 0.0090 - val_loss: 0.0292 - val_acc: 0.0085\n",
      "Epoch 129/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0343 - acc: 0.0110 - val_loss: 0.0291 - val_acc: 0.0116\n",
      "Epoch 130/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0343 - acc: 0.0095 - val_loss: 0.0291 - val_acc: 0.0095\n",
      "Epoch 131/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0344 - acc: 0.0098 - val_loss: 0.0291 - val_acc: 0.0100\n",
      "Epoch 132/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0344 - acc: 0.0105 - val_loss: 0.0291 - val_acc: 0.0121\n",
      "Epoch 133/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0344 - acc: 0.0099 - val_loss: 0.0290 - val_acc: 0.0105\n",
      "Epoch 134/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0343 - acc: 0.0096 - val_loss: 0.0290 - val_acc: 0.0090\n",
      "Epoch 135/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0342 - acc: 0.0092 - val_loss: 0.0289 - val_acc: 0.0100\n",
      "Epoch 136/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0342 - acc: 0.0116 - val_loss: 0.0289 - val_acc: 0.0100\n",
      "Epoch 137/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0344 - acc: 0.0096 - val_loss: 0.0289 - val_acc: 0.0100\n",
      "Epoch 138/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0343 - acc: 0.0109 - val_loss: 0.0289 - val_acc: 0.0105\n",
      "Epoch 139/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0342 - acc: 0.0110 - val_loss: 0.0288 - val_acc: 0.0085\n",
      "Epoch 140/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0342 - acc: 0.0112 - val_loss: 0.0288 - val_acc: 0.0085\n",
      "Epoch 141/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0342 - acc: 0.0098 - val_loss: 0.0288 - val_acc: 0.0095\n",
      "Epoch 142/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0342 - acc: 0.0116 - val_loss: 0.0288 - val_acc: 0.0100\n",
      "Epoch 143/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0342 - acc: 0.0101 - val_loss: 0.0288 - val_acc: 0.0100\n",
      "Epoch 144/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0343 - acc: 0.0103 - val_loss: 0.0287 - val_acc: 0.0110\n",
      "Epoch 145/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0341 - acc: 0.0105 - val_loss: 0.0287 - val_acc: 0.0105\n",
      "Epoch 146/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0342 - acc: 0.0121 - val_loss: 0.0286 - val_acc: 0.0095\n",
      "Epoch 147/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0341 - acc: 0.0095 - val_loss: 0.0286 - val_acc: 0.0080\n",
      "Epoch 148/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0341 - acc: 0.0091 - val_loss: 0.0286 - val_acc: 0.0105\n",
      "Epoch 149/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0341 - acc: 0.0095 - val_loss: 0.0286 - val_acc: 0.0100\n",
      "Epoch 150/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0340 - acc: 0.0109 - val_loss: 0.0285 - val_acc: 0.0100\n",
      "Epoch 151/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0340 - acc: 0.0098 - val_loss: 0.0284 - val_acc: 0.0121\n",
      "Epoch 152/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0340 - acc: 0.0103 - val_loss: 0.0285 - val_acc: 0.0105\n",
      "Epoch 153/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0340 - acc: 0.0089 - val_loss: 0.0284 - val_acc: 0.0121\n",
      "Epoch 154/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0340 - acc: 0.0101 - val_loss: 0.0284 - val_acc: 0.0121\n",
      "Epoch 155/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0340 - acc: 0.0095 - val_loss: 0.0284 - val_acc: 0.0116\n",
      "Epoch 156/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0341 - acc: 0.0109 - val_loss: 0.0284 - val_acc: 0.0105\n",
      "Epoch 157/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0340 - acc: 0.0101 - val_loss: 0.0284 - val_acc: 0.0110\n",
      "Epoch 158/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0340 - acc: 0.0092 - val_loss: 0.0283 - val_acc: 0.0105\n",
      "Epoch 159/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0340 - acc: 0.0113 - val_loss: 0.0283 - val_acc: 0.0116\n",
      "Epoch 160/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0340 - acc: 0.0091 - val_loss: 0.0283 - val_acc: 0.0116\n",
      "Epoch 161/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0339 - acc: 0.0098 - val_loss: 0.0282 - val_acc: 0.0105\n",
      "Epoch 162/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0338 - acc: 0.0095 - val_loss: 0.0282 - val_acc: 0.0116\n",
      "Epoch 163/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0340 - acc: 0.0095 - val_loss: 0.0282 - val_acc: 0.0110\n",
      "Epoch 164/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0337 - acc: 0.0100 - val_loss: 0.0281 - val_acc: 0.0121\n",
      "Epoch 165/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0339 - acc: 0.0114 - val_loss: 0.0281 - val_acc: 0.0116\n",
      "Epoch 166/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0339 - acc: 0.0087 - val_loss: 0.0281 - val_acc: 0.0100\n",
      "Epoch 167/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0337 - acc: 0.0109 - val_loss: 0.0281 - val_acc: 0.0121\n",
      "Epoch 168/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0338 - acc: 0.0113 - val_loss: 0.0281 - val_acc: 0.0105\n",
      "Epoch 169/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0337 - acc: 0.0094 - val_loss: 0.0279 - val_acc: 0.0100\n",
      "Epoch 170/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0338 - acc: 0.0106 - val_loss: 0.0280 - val_acc: 0.0105\n",
      "Epoch 171/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0338 - acc: 0.0092 - val_loss: 0.0280 - val_acc: 0.0105\n",
      "Epoch 172/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0337 - acc: 0.0094 - val_loss: 0.0279 - val_acc: 0.0110\n",
      "Epoch 173/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0338 - acc: 0.0104 - val_loss: 0.0278 - val_acc: 0.0110\n",
      "Epoch 174/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0336 - acc: 0.0087 - val_loss: 0.0279 - val_acc: 0.0100\n",
      "Epoch 175/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0337 - acc: 0.0109 - val_loss: 0.0278 - val_acc: 0.0110\n",
      "Epoch 176/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0337 - acc: 0.0101 - val_loss: 0.0278 - val_acc: 0.0105\n",
      "Epoch 177/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0336 - acc: 0.0091 - val_loss: 0.0278 - val_acc: 0.0100\n",
      "Epoch 178/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0336 - acc: 0.0108 - val_loss: 0.0277 - val_acc: 0.0116\n",
      "Epoch 179/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0336 - acc: 0.0100 - val_loss: 0.0277 - val_acc: 0.0105\n",
      "Epoch 180/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0336 - acc: 0.0099 - val_loss: 0.0277 - val_acc: 0.0116\n",
      "Epoch 181/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0335 - acc: 0.0094 - val_loss: 0.0276 - val_acc: 0.0100\n",
      "Epoch 182/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0335 - acc: 0.0101 - val_loss: 0.0276 - val_acc: 0.0100\n",
      "Epoch 183/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0336 - acc: 0.0101 - val_loss: 0.0276 - val_acc: 0.0110\n",
      "Epoch 184/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0336 - acc: 0.0098 - val_loss: 0.0275 - val_acc: 0.0100\n",
      "Epoch 185/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0335 - acc: 0.0109 - val_loss: 0.0276 - val_acc: 0.0110\n",
      "Epoch 186/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0335 - acc: 0.0085 - val_loss: 0.0276 - val_acc: 0.0100\n",
      "Epoch 187/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0335 - acc: 0.0094 - val_loss: 0.0275 - val_acc: 0.0100\n",
      "Epoch 188/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0335 - acc: 0.0094 - val_loss: 0.0275 - val_acc: 0.0110\n",
      "Epoch 189/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0335 - acc: 0.0106 - val_loss: 0.0275 - val_acc: 0.0105\n",
      "Epoch 190/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0334 - acc: 0.0116 - val_loss: 0.0275 - val_acc: 0.0105\n",
      "Epoch 191/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0333 - acc: 0.0100 - val_loss: 0.0274 - val_acc: 0.0116\n",
      "Epoch 192/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0335 - acc: 0.0101 - val_loss: 0.0274 - val_acc: 0.0095\n",
      "Epoch 193/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0332 - acc: 0.0103 - val_loss: 0.0274 - val_acc: 0.0110\n",
      "Epoch 194/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0334 - acc: 0.0100 - val_loss: 0.0273 - val_acc: 0.0100\n",
      "Epoch 195/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0335 - acc: 0.0107 - val_loss: 0.0273 - val_acc: 0.0095\n",
      "Epoch 196/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0334 - acc: 0.0097 - val_loss: 0.0273 - val_acc: 0.0110\n",
      "Epoch 197/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0334 - acc: 0.0098 - val_loss: 0.0273 - val_acc: 0.0095\n",
      "Epoch 198/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0332 - acc: 0.0116 - val_loss: 0.0272 - val_acc: 0.0105\n",
      "Epoch 199/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0333 - acc: 0.0092 - val_loss: 0.0272 - val_acc: 0.0100\n",
      "Epoch 200/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0332 - acc: 0.0101 - val_loss: 0.0272 - val_acc: 0.0090\n",
      "Epoch 201/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0333 - acc: 0.0099 - val_loss: 0.0272 - val_acc: 0.0110\n",
      "Epoch 202/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0334 - acc: 0.0095 - val_loss: 0.0272 - val_acc: 0.0105\n",
      "Epoch 203/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0332 - acc: 0.0110 - val_loss: 0.0271 - val_acc: 0.0105\n",
      "Epoch 204/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0332 - acc: 0.0101 - val_loss: 0.0271 - val_acc: 0.0116\n",
      "Epoch 205/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0332 - acc: 0.0097 - val_loss: 0.0271 - val_acc: 0.0105\n",
      "Epoch 206/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0332 - acc: 0.0109 - val_loss: 0.0271 - val_acc: 0.0110\n",
      "Epoch 207/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0334 - acc: 0.0100 - val_loss: 0.0272 - val_acc: 0.0105\n",
      "Epoch 208/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0332 - acc: 0.0097 - val_loss: 0.0270 - val_acc: 0.0090\n",
      "Epoch 209/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0331 - acc: 0.0075 - val_loss: 0.0270 - val_acc: 0.0105\n",
      "Epoch 210/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0331 - acc: 0.0101 - val_loss: 0.0269 - val_acc: 0.0095\n",
      "Epoch 211/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0331 - acc: 0.0103 - val_loss: 0.0271 - val_acc: 0.0090\n",
      "Epoch 212/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0331 - acc: 0.0106 - val_loss: 0.0269 - val_acc: 0.0100\n",
      "Epoch 213/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0331 - acc: 0.0096 - val_loss: 0.0269 - val_acc: 0.0105\n",
      "Epoch 214/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0330 - acc: 0.0078 - val_loss: 0.0269 - val_acc: 0.0095\n",
      "Epoch 215/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0331 - acc: 0.0108 - val_loss: 0.0268 - val_acc: 0.0090\n",
      "Epoch 216/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0331 - acc: 0.0103 - val_loss: 0.0269 - val_acc: 0.0090\n",
      "Epoch 217/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0331 - acc: 0.0089 - val_loss: 0.0269 - val_acc: 0.0080\n",
      "Epoch 218/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0330 - acc: 0.0094 - val_loss: 0.0268 - val_acc: 0.0100\n",
      "Epoch 219/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0330 - acc: 0.0119 - val_loss: 0.0268 - val_acc: 0.0090\n",
      "Epoch 220/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0330 - acc: 0.0095 - val_loss: 0.0268 - val_acc: 0.0100\n",
      "Epoch 221/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0330 - acc: 0.0107 - val_loss: 0.0268 - val_acc: 0.0090\n",
      "Epoch 222/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0331 - acc: 0.0098 - val_loss: 0.0268 - val_acc: 0.0090\n",
      "Epoch 223/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0330 - acc: 0.0080 - val_loss: 0.0268 - val_acc: 0.0100\n",
      "Epoch 224/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0329 - acc: 0.0103 - val_loss: 0.0267 - val_acc: 0.0095\n",
      "Epoch 225/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0330 - acc: 0.0085 - val_loss: 0.0267 - val_acc: 0.0090\n",
      "Epoch 226/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0330 - acc: 0.0111 - val_loss: 0.0267 - val_acc: 0.0095\n",
      "Epoch 227/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0329 - acc: 0.0079 - val_loss: 0.0268 - val_acc: 0.0100\n",
      "Epoch 228/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0330 - acc: 0.0099 - val_loss: 0.0267 - val_acc: 0.0085\n",
      "Epoch 229/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0329 - acc: 0.0092 - val_loss: 0.0267 - val_acc: 0.0090\n",
      "Epoch 230/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0329 - acc: 0.0109 - val_loss: 0.0266 - val_acc: 0.0095\n",
      "Epoch 231/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0329 - acc: 0.0111 - val_loss: 0.0266 - val_acc: 0.0075\n",
      "Epoch 232/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0328 - acc: 0.0098 - val_loss: 0.0265 - val_acc: 0.0100\n",
      "Epoch 233/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0328 - acc: 0.0103 - val_loss: 0.0266 - val_acc: 0.0105\n",
      "Epoch 234/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0328 - acc: 0.0096 - val_loss: 0.0267 - val_acc: 0.0090\n",
      "Epoch 235/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0329 - acc: 0.0099 - val_loss: 0.0267 - val_acc: 0.0095\n",
      "Epoch 236/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0328 - acc: 0.0097 - val_loss: 0.0267 - val_acc: 0.0100\n",
      "Epoch 237/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0327 - acc: 0.0098 - val_loss: 0.0266 - val_acc: 0.0095\n",
      "Epoch 238/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0329 - acc: 0.0095 - val_loss: 0.0266 - val_acc: 0.0085\n",
      "Epoch 239/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0327 - acc: 0.0107 - val_loss: 0.0266 - val_acc: 0.0095\n",
      "Epoch 240/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0328 - acc: 0.0093 - val_loss: 0.0266 - val_acc: 0.0105\n",
      "Epoch 241/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0327 - acc: 0.0092 - val_loss: 0.0265 - val_acc: 0.0090\n",
      "Epoch 242/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0327 - acc: 0.0096 - val_loss: 0.0264 - val_acc: 0.0090\n",
      "Epoch 243/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0328 - acc: 0.0089 - val_loss: 0.0265 - val_acc: 0.0090\n",
      "Epoch 244/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0327 - acc: 0.0095 - val_loss: 0.0264 - val_acc: 0.0095\n",
      "Epoch 245/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0328 - acc: 0.0089 - val_loss: 0.0265 - val_acc: 0.0090\n",
      "Epoch 246/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0326 - acc: 0.0096 - val_loss: 0.0264 - val_acc: 0.0100\n",
      "Epoch 247/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0328 - acc: 0.0098 - val_loss: 0.0265 - val_acc: 0.0110\n",
      "Epoch 248/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0325 - acc: 0.0092 - val_loss: 0.0265 - val_acc: 0.0095\n",
      "Epoch 249/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0326 - acc: 0.0088 - val_loss: 0.0263 - val_acc: 0.0095\n",
      "Epoch 250/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0328 - acc: 0.0090 - val_loss: 0.0264 - val_acc: 0.0116\n",
      "Epoch 251/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0327 - acc: 0.0098 - val_loss: 0.0264 - val_acc: 0.0100\n",
      "Epoch 252/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0326 - acc: 0.0099 - val_loss: 0.0264 - val_acc: 0.0095\n",
      "Epoch 253/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11791/11791 [==============================] - 1s - loss: 0.0327 - acc: 0.0094 - val_loss: 0.0264 - val_acc: 0.0090\n",
      "Epoch 254/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0327 - acc: 0.0109 - val_loss: 0.0264 - val_acc: 0.0090\n",
      "Epoch 255/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0326 - acc: 0.0102 - val_loss: 0.0263 - val_acc: 0.0090\n",
      "Epoch 256/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0327 - acc: 0.0103 - val_loss: 0.0264 - val_acc: 0.0080\n",
      "Epoch 257/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0326 - acc: 0.0107 - val_loss: 0.0264 - val_acc: 0.0090\n",
      "Epoch 258/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0326 - acc: 0.0109 - val_loss: 0.0264 - val_acc: 0.0080\n",
      "Epoch 259/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0326 - acc: 0.0112 - val_loss: 0.0263 - val_acc: 0.0095\n",
      "Epoch 260/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0326 - acc: 0.0092 - val_loss: 0.0263 - val_acc: 0.0090\n",
      "Epoch 261/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0326 - acc: 0.0107 - val_loss: 0.0262 - val_acc: 0.0110\n",
      "Epoch 262/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0326 - acc: 0.0097 - val_loss: 0.0263 - val_acc: 0.0080\n",
      "Epoch 263/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0326 - acc: 0.0103 - val_loss: 0.0264 - val_acc: 0.0095\n",
      "Epoch 264/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0326 - acc: 0.0098 - val_loss: 0.0263 - val_acc: 0.0090\n",
      "Epoch 265/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0326 - acc: 0.0097 - val_loss: 0.0262 - val_acc: 0.0116\n",
      "Epoch 266/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0327 - acc: 0.0086 - val_loss: 0.0262 - val_acc: 0.0085\n",
      "Epoch 267/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0326 - acc: 0.0087 - val_loss: 0.0263 - val_acc: 0.0085\n",
      "Epoch 268/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0326 - acc: 0.0097 - val_loss: 0.0262 - val_acc: 0.0085\n",
      "Epoch 269/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0325 - acc: 0.0100 - val_loss: 0.0262 - val_acc: 0.0090\n",
      "Epoch 270/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0324 - acc: 0.0095 - val_loss: 0.0263 - val_acc: 0.0095\n",
      "Epoch 271/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0325 - acc: 0.0102 - val_loss: 0.0263 - val_acc: 0.0100\n",
      "Epoch 272/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0325 - acc: 0.0110 - val_loss: 0.0262 - val_acc: 0.0090\n",
      "Epoch 273/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0325 - acc: 0.0094 - val_loss: 0.0262 - val_acc: 0.0095\n",
      "Epoch 274/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0325 - acc: 0.0110 - val_loss: 0.0262 - val_acc: 0.0080\n",
      "Epoch 275/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0324 - acc: 0.0097 - val_loss: 0.0262 - val_acc: 0.0110\n",
      "Epoch 276/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0323 - acc: 0.0085 - val_loss: 0.0262 - val_acc: 0.0105\n",
      "Epoch 277/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0325 - acc: 0.0098 - val_loss: 0.0262 - val_acc: 0.0100\n",
      "Epoch 278/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0324 - acc: 0.0102 - val_loss: 0.0261 - val_acc: 0.0100\n",
      "Epoch 279/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0323 - acc: 0.0099 - val_loss: 0.0262 - val_acc: 0.0095\n",
      "Epoch 280/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0325 - acc: 0.0091 - val_loss: 0.0263 - val_acc: 0.0116\n",
      "Epoch 281/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0324 - acc: 0.0111 - val_loss: 0.0263 - val_acc: 0.0110\n",
      "Epoch 282/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0323 - acc: 0.0103 - val_loss: 0.0262 - val_acc: 0.0090\n",
      "Epoch 283/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0324 - acc: 0.0103 - val_loss: 0.0262 - val_acc: 0.0095\n",
      "Epoch 284/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0325 - acc: 0.0097 - val_loss: 0.0262 - val_acc: 0.0100\n",
      "Epoch 285/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0325 - acc: 0.0108 - val_loss: 0.0262 - val_acc: 0.0105\n",
      "Epoch 286/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0323 - acc: 0.0103 - val_loss: 0.0261 - val_acc: 0.0090\n",
      "Epoch 287/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0323 - acc: 0.0091 - val_loss: 0.0262 - val_acc: 0.0110\n",
      "Epoch 288/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0324 - acc: 0.0106 - val_loss: 0.0262 - val_acc: 0.0100\n",
      "Epoch 289/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0323 - acc: 0.0099 - val_loss: 0.0261 - val_acc: 0.0105\n",
      "Epoch 290/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0323 - acc: 0.0099 - val_loss: 0.0261 - val_acc: 0.0100\n",
      "Epoch 291/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0323 - acc: 0.0098 - val_loss: 0.0262 - val_acc: 0.0116\n",
      "Epoch 292/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0322 - acc: 0.0094 - val_loss: 0.0262 - val_acc: 0.0095\n",
      "Epoch 293/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0324 - acc: 0.0093 - val_loss: 0.0261 - val_acc: 0.0095\n",
      "Epoch 294/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0323 - acc: 0.0087 - val_loss: 0.0260 - val_acc: 0.0105\n",
      "Epoch 295/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0322 - acc: 0.0090 - val_loss: 0.0261 - val_acc: 0.0090\n",
      "Epoch 296/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0324 - acc: 0.0101 - val_loss: 0.0261 - val_acc: 0.0105\n",
      "Epoch 297/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0322 - acc: 0.0090 - val_loss: 0.0260 - val_acc: 0.0105\n",
      "Epoch 298/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0323 - acc: 0.0105 - val_loss: 0.0260 - val_acc: 0.0105\n",
      "Epoch 299/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0322 - acc: 0.0089 - val_loss: 0.0260 - val_acc: 0.0095\n",
      "Epoch 300/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0323 - acc: 0.0093 - val_loss: 0.0261 - val_acc: 0.0100\n",
      "Test loss: 0.0261472878666\n",
      "Test accuracy: 0.0100452034154\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train_vec, x_train_vec,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test_vec, x_test_vec))\n",
    "score = model.evaluate(x_test_vec, x_test_vec, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "エポック数は先ほどより長くしたこともあり少々待ち時間がかかる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHEAAADqCAYAAAAlBtnSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xv8VXO+x/FP5JpKISm6qujmFpOiMYkw7jTujBmEGR7H\n4MxxjnGZQZjhuA0GMy6TEyaRBzrEyF2KYtKFitKFSleKpN/5Yx7nM+/vp9/e9u+2f7+19+v512fP\n+rZ/y177u9baa76fz6dRRUWFAQAAAAAAoGHbqL53AAAAAAAAAN+PhzgAAAAAAAAZwEMcAAAAAACA\nDOAhDgAAAAAAQAbwEAcAAAAAACADeIgDAAAAAACQATzEAQAAAAAAyAAe4gAAAAAAAGQAD3EAAAAA\nAAAyoHFVBjdq1KiirnYE+VVUVDSqjffhGNarJRUVFdvVxhtxHOsPc7EkMBdLAHOxJDAXSwBzsSQw\nF0sAc7EkFDQXWYkDFM+c+t4BAGbGXAQaCuYi0DAwF4GGoaC5yEMcAAAAAACADOAhDgAAAAAAQAbw\nEAcAAAAAACADeIgDAAAAAACQATzEAQAAAAAAyAAe4gAAAAAAAGQAD3EAAAAAAAAygIc4AAAAAAAA\nGcBDHAAAAAAAgAzgIQ4AAAAAAEAG8BAHAAAAAAAgA3iIAwAAAAAAkAGN63sHAKAuvfjiix43atTI\n44EDB9bH7gCZ1LVrV4/vvvtuj0855ZRk3MKFC4u2TwAAAOWIlTgAAAAAAAAZwEMcAAAAAACADCCd\nqgYOPPBAjx9++OFk2w9/+EOPZ8yYUbR9Asrdf//3fyev+/Xr5/FDDz1U7N0BzMysadOmyeutttrK\n4xUrVni8evXqou1TVRx22GEeDxgwwOOzzjorGTds2DCP161bV/c7BgAAUGZYiQMAAAAAAJABPMQB\nAAAAAADIgKKkU+nS62222cbjJ554ohh/vs7svffeHk+YMKEe9wQob9dff73H5557brLt22+/9Vg7\nVQHF9O///u/J68suu8zjSy+91OOYDthQTJw4sdL//corr0xejxgxwuOZM2fW6T5hQ+3bt/f4oosu\nSradf/75Hjdu/K/bv0ceeSQZd/LJJ9fR3qE2tGvXzuM333zT48GDByfjpkyZUrR9ArJMz5sx7X7/\n/ff3uKKiItmmHU+nTZvm8QEHHJCMW7x4cW3sJpBgJQ4AAAAAAEAG8BAHAAAAAAAgA3iIAwAAAAAA\nkAFFqYmjuYFdunTxOGs1cTbaKH3m1bFjR481n9IszZNE1fzgBz/w+NRTT/VY27abmfXo0SPne1xy\nySUeL1iwwOP99tsvGTd8+HCPx48fX/WdRYPQt29fjzfZZJNk22uvvebxY489VrR9KjUtW7b0+IQT\nTki2/ed//qfHbdq0yfkel19+ucfairrcaV2Z2bNnJ9tGjx5d7N2pVOvWret7F1CJM888M3l9yy23\nePzRRx8l24YOHerxTjvt5HGsa/Tb3/7W4+nTp9fKfpaTrl27evz1118n2+bOnVvj97/rrrs8Xrt2\nrcerVq2q8Xuj+nr27Jm87t+/v8d6zCL9vfDcc88l22699VaPx4wZU9NdhNhll108vvbaaz3W42aW\n1sGJNXFUt27dPI51dQ499NBq7ydya9KkicejRo3y+OCDD07GrV+/Pud7fP755x7/+c9/zjnuvvvu\n83jOnDlV2s+6wkocAAAAAACADOAhDgAAAAAAQAYUJZ3q9NNP91jbIWbNDjvskLw+++yzPda0HDOW\nIFdFTM3Q5aPbbrutxzFFbdy4cR5vt912ybbf//73lf6t+B7670488cTCdhgFGTBggMf/9V//5fFJ\nJ52UjFu6dGmV3zu+hy5jnjVrVrJNU+tQNZqmpq2v99lnn2RcocuNf/e733msKQdmG6aFlJOtttrK\n4/vvvz/ZpsuCc7X5rut9MjP71a9+VdC/GzJkiMekzNWeTTfd1OOLL77Y4yuuuCIZd/PNN3scr4PL\nly/3eM899/Q4plORllN1xxxzjMcPPvigx/Gz1fNoofQ8bGY2aNAgj6+//nqPG8oS/1Kn57jjjz/e\n48MPPzwZt/nmm3uc77qo2w466KBkW+/evT3W+yizDa8VyE/LM5iZ3XTTTR5vueWWHk+aNCkZd++9\n93qsKTtmZn369PH4mWee8VjbjaNmtthiC4/1d4WZ2d/+9jeP9Rh+9913ybiFCxd63Lhx+uijVatW\nHl922WU592PnnXf2OP4GqS+sxAEAAAAAAMgAHuIAAAAAAABkAA9xAAAAAAAAMqAoNXFia+6s0vZi\nUWzliQ1pHqLmkWq+qVma1/jKK694rPU0zNLW0ZtttlmyTVtJx1Zzqpg1JsrNPffc43GXLl087t69\nezJOj2OhtKW1mdk222zjsdaqMjN77733qvz+5UprUJmlc3PXXXf1ePHixcm4J5980uPYEltromkt\ngVjnQWt+aNvcUvHJJ58UNK5Zs2bJ66uvvtrjmNO/bNmyGu9XLpr/bbZhHSQUl9aMuuaaazz+t3/7\nt2Tc7bffXtD76XVx0aJFybb58+dXZxfL2imnnOKxng+rUwMnOvroo5PXei/1+OOP1/j9kZ8eW7O0\n7pReM//6178m47RduF7fzMxuuOEGj2NNR7X99tt7vOOOOxa4x6iM1sAxS897WlvsiSeeSMbp8Tn2\n2GOTbfrd0Ho51113Xc12tszpd/3GG2/0+Cc/+UnOf6P3QxdddFGyTevWxnusq666yuMLL7ywoPdv\nKErj6QoAAAAAAECJ4yEOAAAAAABABtRJOpW2xDNLlwNmWfPmzXNuGzt2bBH3JJs0FSBfapp+ltp+\nfOXKlTn/TWxTniuFat68eclrbQWK2rV69WqPtYWmtt2sit13393j9u3bJ9vWr19f4/fHhqlQmkL1\n/PPPe3zYYYcV/J6aaqqtcePScP1bpZgC98ADDySv27Rp43FsQ6wGDx7s8XHHHZdsy3ceramYYjN7\n9myPO3XqlPPfactPVF/Lli2T15pOPHLkSI/vuuuugt9Tz5tnnXVWDfYOUf/+/T2OaTU1pecKM7NG\njRrV6vsjv5/+9KfJa02h+o//+A+Pb7vttmTcN99843FMp9Lr3fnnn+9xkyZNknF6b7Nq1aoq7DXM\n0rbsMW1N059iCpXSY6Wp+2Zm+++/v8d/+tOfPF6yZEnVd7aMbbXVVslrTUnVe/+lS5cm4/QY3nHH\nHR5PmTIl59+K9y/xvur/jRkzJnkdyzg0BKzEAQAAAAAAyAAe4gAAAAAAAGRAnaRTxaX2W2yxRV38\nmaLQVLCOHTvmHEc3hw3FblK6FE3Ta+68885k3OWXX+5xvhQqpUsm84mVx2OXHVRfPN69evXyeNq0\naR5XJVVGlxb/+te/9lg7mJmZvfXWWx5rqgGqZs2aNTm3xVSrmopzu9SXH3/33XfJa116rx0uYlco\n9Ytf/CJ5rUvAv/jii5ruYqJVq1bJ63wpVKgd2nXo9ddfT7Z9/vnnHp933nker1u3ruD31w4dejxj\n1xZ8v5jipGm8en9TG+Jy/6+++srjr7/+ulb/FjbUtGnTnNt0/n377bfJtsMPP9zj+J3o16+fxzGF\nSumcveWWW75/Z5HQzm7VnZfaJfeyyy5Ltul7Tp8+vVrvX640hSqmBWsKld4bHn/88cm4Qrvbajpj\n/K3Stm3bSv+NdpAzM1u+fHlBf6uYWIkDAAAAAACQATzEAQAAAAAAyAAe4gAAAAAAAGRAndTE6dat\nW85tH3zwQV38yTrzhz/8wePYKv3DDz/0mNZ//3TFFVd4HNuxrV271uPnnnvOY611Ypa7LkdsHa1t\nxNu1a5ds0xac11xzjce1Xdej3O20004en3322ck2zRX/5S9/6XFV6hDdfPPNHg8ZMsTjBQsWJOO0\nvSuqL7au1dfLli3zOM7Fzp07exzbse61114ef/bZZx6fdNJJybhyqyu2YsUKj7X+Sb6aOFpnyiyd\nf4XWxNHc8KFDh+Ycp/MNxaH5/l27dk22DRw40OPYZjWXOMf69u3r8Zdffumx3uegMPEapOdHrdm2\n2WabJeO07XShYl3JyZMnezxr1qxa/VvYkNajivSe91e/+lWyTWsXdenSpaC/pa2VzTasg4aqOe20\n0zyeOnVqsm3AgAEe33333R7HduPnnHOOx/q7w8zs008/9fjhhx+u2c6WGb3XidcqddZZZ3lcaA2c\nnj17Jq/vvfdej/fee+9Cd7HBYyUOAAAAAABABvAQBwAAAAAAIAPqJJ0qnwkTJhT7T1aqWbNmHh9y\nyCHJtlNPPdXjuHROaZuyhth6rBi23nrr5PX555/vcWznpylU2vYvH11uF5cqappGpG2mb7zxxoL+\nFgqjyxR12em2226bjLv99ts9fvnllwt670suuSR5HVNz/t+1115b0Puhanr06JG81jmsS8Uvvvji\nZFy+uXjiiSd6TPv3yr355psen3HGGQX/u3333ddjTbHQ9rXxtbb1vPzyy6u0n5WZNm1a8lrTSlA1\neuxnzJiRbHvjjTcKeo/WrVt7HFsSb7TRv/5/Oz0/50sXQWH03Kbnx1atWiXjtEXx7Nmzq/W3tD38\nCy+84PF1112XjBs7dmy13h8pva81S0sraGqG/q6oLr1PNjNbvXp1jd+znGnb71GjRiXb9HfILrvs\n4rGm75ilaeXxd42WB9BW2Ph+u+66a85tCxcu9HjKlCkFvd/Pf/5zj7WMhll63/PJJ58k2zp06ODx\npEmTPNZ7qoaKlTgAAAAAAAAZwEMcAAAAAACADCh6OlXLli2r9e922203j3Vp26BBg5JxO+64o8fa\nheOUU05Jxumy4tgNafz48R5rdf/GjdOP65133ilo30uZfsZmG6bUqAsvvNBjXWJ85plnJuOOPPJI\njzV1R5fDmaXLGuMSx+HDh3usHQJQGP2ua3qhmdmf//xnj3UerV+/PhmnqR66hFw7Tpml54TYEUfn\n+kMPPeTxn/70p/z/AaiW2OGoadOmHvfp08fj2MVK519c/h07QmBD9913n8c//OEPk20nn3xyzn93\nxx13VBrnk2/OVkf37t2T17pEXc8V+H6DBw/2WLvemJl9++23lf6bmMLx+OOPexyvx9qB5YYbbqj2\nfmJDw4YN81jTwOM17Sc/+YnHjz32mMdff/11Mk5TpuL5Vo/5U0895THpU3Ujdk7UrkbaqTEepxNO\nOMHju+66K+f7673N/fffX+39RH7a/c/M7JhjjvH42GOP9VhTq8zMmjRp4nHsvqxdj1A1+cpqaGra\nj370o5zjLrjgAo/193/s6KedPX/zm98k2zSdStOWtYNjQ8VKHAAAAAAAgAzgIQ4AAAAAAEAG8BAH\nAAAAAAAgAxrFWiJ5BzdqVNDgO++8M3k9dOhQj7UV99y5cwv+271799b98HjdunXJOK3HoLUYtM6N\nmdnEiRM9ju2Ptd3mvHnzPG7RokUyLtaDqUsVFRWNvn/U9yv0GBYqthjXdrPbbbdd/NseF/q9W7Bg\nQaX/3sxshx128Fjb/MVtDcg7FRUVfb5/2Per7eMYaR2cBx54IN9+eDxz5sxkW+fOnSv9Nzr3zMza\ntm3rcTxuelwbyjFtqHOxLvTt29djzTd+9NFHk3E6n7UelVnuNvH1rMHOxd133z15HedLTVXnPFwV\nWtPh7LPPrvX3V6UwFw888ECPtaaJ3vOYpW1WtXZOrA/Wrl07j+M5Wdshr1ixopp7XOsa7Fysro03\n3thjrYFjltbl6NixY8730PvN9u3bJ9uOOuooj8eMGeNxvB8uplKYizUVa6k8+eSTHnfp0iXZpr87\ntG5SrEtXZCU3F2vDd99953G8Zmr7+Xvuuado+5RPVubifvvt5/G4ceNq/H56r6S1Vc3S2qixdpie\nh7X+jranrwcFzUVW4gAAAAAAAGQAD3EAAAAAAAAyoE5ajOvyMjOzOXPmeNyvX79qvaemXukSRU3f\nMTN76623qvX+6pxzzvFYU4Jmz55d4/cuNZoeZ5a2jHv66aeTbdpKetasWR6PHj06GafpO0uXLvX4\nkUceScZpek3chqrRVphmaUpEbG2rx1zbHy9btiwZd9NNN3msbZO1VbVZ/vQObZH76aefenzAAQck\n4/T7hNqj59OePXsW9G+uu+66utod1AJNsYnz7ZlnnvE4ptvElteoHZq+rW2mtf20mVnTpk091vuS\nb775Jhmn59M//vGPybYGlEJV0jT9YsSIEcm2+DqX0047zeMHH3ww2fb22297XJ8pVEjTFy+66KJk\nm6ZQxfuoSy+91ON6TqFCoK3HzdJzqra+Nms4KVRZpL+phw0blmzT85+m8ke33Xabx9o6XNOnzNJj\nus8++yTbJk+e7HE9p1BVGStxAAAAAAAAMoCHOAAAAAAAABnAQxwAAAAAAIAMqJOaONENN9xQjD9T\na7Tlp3r88ceLvCfZo63cY4vx6hgwYIDHWlfFzGz9+vUeU6+oZoYOHZq81hpU11xzTbJN6+Xkc8EF\nF3isbXD33XffgvdLc5Ffeuklj6mBU3y9evXyeKON0uf/OhdRP7R+mM5fs7Q+VaE1OWKrc2ri1A1t\nHX7uued6/POf/zwZ995773msx/COO+5Ixmmb1dh+HNnRqVOn+t4FFEDr85111lnJNq05dtVVVyXb\nuGdtWLQ9/EMPPZRs0+OotVpQMwsWLPBY69lU9rqqtIacmdnIkSNzjn355Zdr9LfqEytxAAAAAAAA\nMoCHOAAAAAAAABlQlHSqUvHEE0/U9y6UnS222MLjmLKhSxxpMV4zsc37qFGjPNbW3lWh7cHztac+\n6aSTPNbUgmjevHnV2g/UjjVr1ngc5+K4ceM8Xrt2bbF2qSTFZfa6tDumWEybNs1jbSedbx7VtYMP\nPtjjFi1aJNuWLVtW7N3JFD3WcUm/ppbecsstHm+//fbJuGOPPdZjbVmOhm2zzTZLXh9xxBEex/m8\ncuXKouwTKrfbbrt5fN9993ms96RmZnfddZfH119/fd3vGKqtQ4cOHm+55ZbJtnfffdfj559/vli7\nhBoYNGhQ8lrn5ieffJJs03maNazEAQAAAAAAyAAe4gAAAAAAAGQA6VRo0J577rn63oWycOutt9b4\nPZo3b568HjJkiMfNmjXzOHaWeuyxx2r8t1E3tGODdstZvHhxMk6Xo8alqqiamCrxs5/9rJ72pHra\ntm3r8aabblqPe1JatDvjL3/5S4+vvfbaZJx2p0J2xNTDPfbYw+PY4VVTW1H3dthhh+T1sGHDPN54\n44091rQbs7RDJxoevb958MEHPY5pcdddd13R9gm148orr8y5LXbcnTlzZl3vTp1hJQ4AAAAAAEAG\n8BAHAAAAAAAgA3iIAwAAAAAAkAHUxPke2taza9euyba33nqr2LtTdgYPHlzfu4ACnX/++cnr8847\nz+NFixZ5PHDgwKLtE6om1jXSmlRa6+TXv/51Mm7kyJF1u2MouuXLlyevFy5c6HGsEZFLrCUwdOhQ\nj9etW1eDvSs///M//+PxggULPL7xxhvrY3dQy3784x/n3Pb4448XcU8QPfPMM8lrbTH+8ccfe6zn\nNzR8xx13nMfbbbedx7Hm3xNPPFG0fUL1tW7d2uNevXol29auXevx0qVLi7ZPdY2VOAAAAAAAABnA\nQxwAAAAAAIAMIJ3qe2iruY024plXsXXq1Km+dwF5tG/f3uOzzjor2aZz55577vF43rx5db9jqJaY\nmqEpVCNGjPD4pptuKto+oX7EVvHHH3+8x6NGjfJ4++23z/keZ5xxRvL6wgsv9Jh0qvz69OmTvN52\n22091s/xyy+/LNo+oe7sueeeObe98847RdyT8qXtwk866SSPe/funYxbvXq1x3otjC3G0bBoypRZ\nes+q96u0FM+mM888M+e2yZMne/zUU08VY3eKgqcSAAAAAAAAGcBDHAAAAAAAgAzgIQ4AAAAAAEAG\nUBOnCvbdd9/k9QMPPFA/O1JGXn31VY9jTaL169cXe3cQjB071mOtj2NmNnz4cI+vvPLKou0TqmbQ\noEEen3rqqcm2NWvWeEwb8fI2fvx4j4866iiPn3766WSc1m6JtM7Lyy+/XIt7Vxo233xzj7WOmJnZ\n/PnzPf7rX/9atH1C3dFW1eedd16y7fXXXy/27pQ9PR5aF+Wrr75Kxl1++eUe33XXXXW/Y6gVd999\nd/K6Xbt2Ht96662VxmjYOnTo4LHW4NO6VWZpO/lSwkocAAAAAACADOAhDgAAAAAAQAaQTvU9GjVq\nVN+7UNamTJni8UcffZRs0/bjnTt3TrYtXry4bncMZmZ2//33e/y73/0u2TZ69Ohi7w4KpEtQH330\n0ZzjTj/9dI85nvh/EydO9Piiiy5Ktl166aUeP/PMMzn/HTakLVI1tSO+jukdyKYWLVp4rC2OzZgr\nxbDLLrskr5977jmPW7Zs6fEf/vCHZNzf/va3ut0x1JpjjjnG46OPPjrZNnXqVI9pK55NTzzxhMc7\n77yzx/EauWDBgqLtUzGxEgcAAAAAACADeIgDAAAAAACQAaRTVWLMmDEeDxkypB73BCoud7zvvvs8\nvvbaa5NtF1xwgce6ZBK1a9iwYZXGaFi22GKL5PXFF1/scfPmzT1+/PHHk3G6VBWozIgRI/K+RuH0\nuvX+++8n26ZPn17s3UEdO+ywwzz+4osvkm1XXHFFsXenLOj17pFHHkm2aQqVds+74447knGfffZZ\nHe0dakOTJk08vuaaazyOHW6ffPJJj5csWVL3O4Zq0fT/OGe7d+9e6b/5xS9+UZe71GCwEgcAAAAA\nACADeIgDAAAAAACQATzEAQAAAAAAyABq4lTigQceqDRG/Ro1alTy+sQTT/R40KBBybarrrrKY23b\nSmtWlKOf/vSnyevzzz/f4zfeeMNjbSkOoLi0JsfVV1+dbFu3bl2xdwdFNGPGjOT1qlWr6mlPSku7\ndu2S1y+88ILHrVq1Srb98Y9/9PiSSy7xeO3atXW0d6gL2jq+W7duHmudIzOze++9t2j7hOrbe++9\nPe7Tp0/OcbfddpvHw4cPr9N9aihYiQMAAAAAAJABPMQBAAAAAADIgEYVFRWFD27UqPDBqFUVFRWN\nauN9SukYNmvWzOPYYvy8887zuHfv3h7Xc7vxdyoqKnKvBayCUjqOWZOVubjPPvt4HFuH/+Uvf/FY\nlxTPmzevLnepIWEuloCszEXkxVwsAQ1pLjZu/K9KEffdd1+ybbPNNvP4/vvvT7Y9//zzNf3TWcdc\nLAENaS7WhiFDhng8YsSIZNvEiRM9PvDAAz0ugdIZBc1FVuIAAAAAAABkAA9xAAAAAAAAMoCHOAAA\nAAAAABlATZyMKLUcxzJFvnEJYC6WBOZiCWAulgTmYglgLpYE5mIJYC6WBGriAAAAAAAAlAoe4gAA\nAAAAAGRA4+8fklhiZnPqYkeQV/tafC+OYf3hOGYfx7A0cByzj2NYGjiO2ccxLA0cx+zjGJaGgo5j\nlWriAAAAAAAAoH6QTgUAAAAAAJABPMQBAAAAAADIAB7iAAAAAAAAZAAPcQAAAAAAADKAhzgAAAAA\nAAAZwEMcAAAAAACADOAhDgAAAAAAQAbwEAcAAAAAACADeIgDAAAAAACQATzEAQAAAAAAyAAe4gAA\nAAAAAGQAD3EAAAAAAAAygIc4AAAAAAAAGcBDHAAAAAAAgAzgIQ4AAAAAAEAG8BAHAAAAAAAgA3iI\nAwAAAAAAkAE8xAEAAAAAAMgAHuIAAAAAAABkAA9xAAAAAAAAMoCHOAAAAAAAABnAQxwAAAAAAIAM\naFyVwY0aNaqoqx1BfhUVFY1q4304hvVqSUVFxXa18UYcx/rDXCwJzMUSwFwsCczFEsBcLAnMxRLA\nXCwJBc1FVuIAxTOnvncAgJkxF4GGgrkINAzMRaBhKGgu8hAHAAAAAAAgA3iIAwAAAAAAkAE8xAEA\nAAAAAMgAHuIAAAAAAABkAA9xAAAAAAAAMoCHOAAAAAAAABnAQxwAAAAAAIAM4CEOAAAAAABABvAQ\nBwAAAAAAIAN4iAMAAAAAAJABPMQBAAAAAADIgMb1vQO5NGrUKOdrjSsqKpJx+jpuQ/mI359C8H3J\nro02+tfz6M022yzZ1qxZM4/Xrl3r8apVq5Jx69atq6O9AwoXz1363dbYLPf1rjaui/FvNW5c+e3C\nd999l7xev359lf8WGpZCr58cXwAA6gcrcQAAAAAAADKAhzgAAAAAAAAZUK/pVHHJ7sYbb+zxlltu\nmWzT15tssonHcTmvpkusWbPG42+++SYZl2/Jty4j132K+5tvKfG3337rcVxujvSzjMv09TPXY5Fv\niXe+pf+bbrqpx/GYffnllx5//fXXyTb9jqBmNMVJj4dZOjd13kR6jON3ZptttvF4zz33TLa1adPG\n408//dTjCRMmJOO++OILj0kT2JDOP52jZunnFedNrn8X56z+u3wpOqUiVwpg8+bNk3FbbbWVx3Hu\n6PXuq6++8nj16tXJOJ1XmjYYP1c9Pk2bNk226RzTY7pixYpknKYp6v7F/WCOVZ8eJ70fimKKqB43\n/XcxBXXzzTf3OJ6TV65c6XE8vvinXJ9z3FboeTNe7/Q99fwQ73OXLVuW8/1RO/Q4xWuaiue7XCUi\n4rVVX8f30OtkvvM6vp9+zvnub+K9iR7zfL9NdW5y7asbueZUlO/zz1eupdD3KCZW4gAAAAAAAGQA\nD3EAAAAAAAAyoOjpVPmWDery7VatWiXbNCVCl3XH91C6vHzp0qXJtuXLl1c6zixdIlzo8qy4fFGX\nlOvS9nJe5phrqf6OO+6YjGvdurXHeqxbtmyZjNMl3zEVSj9/Xe6/aNGiZNysWbNyvgdqRpfob731\n1pX+72b554TOuVzvZ2bWvXt3j3v27Jls03SAxYsXe5xv6TP+SZfxb7vtth536NAhGde1a1ePd955\n52SbLvfX8+706dOTcR988IHHs2fPTrbp3Gwoy1hrSlOj9Dyn5z+z9Py4xRZbJNv0u63LtWOKk75e\nsmSJx5pOapaeo/V459sPTa8xS1MW4/lWl6KTZlw1ulRfj03btm2TcXqc4jadi/r5a9qNWXoM4zyN\n3y38kx5LPp6XAAAgAElEQVQfvVfZbrvtknF636LXvngd1GucHrdI5/DHH3+cbIvHFbVDz936W6V9\n+/bJuE6dOnncuXPnZJumzepvBD0/m6W/XWbMmJFs++STTzzOlw5eKtfMmoq/4Zo0aeKxnjf1fsYs\nPXbbb799sk3vIxcuXOjx+PHjk3GTJk3yWEt9oGpiemqLFi081vvSeI+q91XxuYHely5YsMDjefPm\nJeP0uhjvnerrtz2/YgAAAAAAADKAhzgAAAAAAAAZwEMcAAAAAACADCh6TRzNH4z5/TvssIPHe+21\nV7Jtl1128Vhz4GKuZ67c0rlz5ybjNKc15vRrbQHNnYvtXbVGh9ZgMUtrFWg9h3KuiaP5pz169PB4\nwIAByTitqaH55PlqqcQcUz2mWgdl6tSpyTjNf0TNxHzjZs2aeazHVP93szTnW/O6zdK2mZpDHs8P\n2lZ8yy23TLZ99NFHHut3gRpIG4rHUGtvHHrooR4fccQRyTjNP445y7layO+2227JuJEjR3occ5FL\nIYc8frZ6PtM8e62jYJZe+2I+vn62+t3WWglm6Xdda6HE66fW69DrsVl6jPW8HOtu6N+K85naDIWL\n3xetkdK/f3+P999//2Rc/I4oPfbxPKlefPFFj7VWldmGbcvLVb7rXe/evT2ONdp0fuj9R5wrWsMh\n1ujQYzdnzhyP47xnvtWOWEOjXbt2Hg8aNMjjOBf1nBnnmx6bfK3m9bfL008/nWzTGh34fnp9M0uv\nrYcccojHP/jBD5JxWvdI6wSapTVUtV5YrGP14YcfehzvPZmn+WkNVb3XNzMbOHCgx/vss4/HOkfN\n0mMfr2F67tW6Yu+9914y7tlnn/VYj6dZ+j0oJlbiAAAAAAAAZAAPcQAAAAAAADKgKOlUuuw0VxtG\ns7RNsC4XNktb9+lyw7h8VJfh67bPP/88GafL+uPyf112pUtkNY3LLE0P0iWtZmlr1XytyUtZviWo\nxxxzjMf77bdfMk6Xneoyt3wpFjFFR9sF6lLIuIxRj2G5HqfaEpeq6nHVVJw4F//+9797HNPb9Hhp\nak9cXt6xY8ec76/LI3VpckzRYUlrmrJmZnbWWWd5fMYZZ3gc27PPnDnT47jMVM+v3bp18zi2idd0\n1VJsP50v/ULTk/QzMkuXD2srcrN0vujS+s8++ywZp+dRvfbFfdI5HFuda5qXXo9jGkhsc6yYY4WL\n59PDDz/c4zPPPNPjmGasbYh1Xpqlc3H33Xf3OF4/NWWgFFIZ60I8B3bp0sXjww47zOPY6lbvY3Su\nxNbSWm4g39/Se5jnn38+Gcd8qx16P2lmNnToUI91XsbjNHv2bI9jaoaee/WcH1vS6/dAz91m6fHl\n/rVyuT5nM7PzzjvP4z59+nisKf5mZv/4xz88/uqrr5Jterz0t0a8VsffQ8hPr0mavn/66acn4zQl\nTo91PJ/q84A4j/Rv6e/UeMy0NEO8zyGdCgAAAAAAADnxEAcAAAAAACADeIgDAAAAAACQAUWviaP1\nTrSOhVnaHqxXr14532/ixIkejxs3LtmmeWqa15ivnXDMKddcRs2h3HXXXZNxmo8a8+/0v7lc85Jj\niz3NEx88eLDH2j7OzGzWrFkeT5o0yePY6lRrI8XvS5s2bSp9f81ZNUu/j+QUV51+Zn379k22XXbZ\nZR7rnHrwwQeTcVrLQ9skm6U1HDT3WGsCmKX1VGKdrHfffddjPSfQKvefNO93yJAhybYLLrjAY62T\nEWsvPPbYYx7HmkTaSlznZfz8Fy5cmHNbKcrVYlxzvOPrmMud6/yotRjM0jx+zd2O5zyt6RBb4u60\n004ed+7cOec4fR2vfVwX89PPp1+/fsm2iy66yGNt/641xczMnnrqKY9jHbk99tjDY60Boa3qzdJz\nMu1wKxfrCGmr6X333dfjOGdff/11j/Va9eWXXybjVq5c6XGstaLX09WrV3vMPUzt0fPYueeem2zT\nuhz6mb/yyivJOJ2LsR34zjvv7LHel8ZaWPPnz/c43h/lqsPBHP0XrfN3zjnnJNt0zup9i7aSNkvr\nGWmNIrP0PK3fmeXLlyfj9J6G47MhvYc3S+toXnjhhR5rPT6z9HOeMGGCxy+//HIyTs+18Z7lRz/6\nkcf6HCLWp9I6gbGWbn3d27ASBwAAAAAAIAN4iAMAAAAAAJABRUmn0mVH2ia4R48eyThtpRqXqk6e\nPNnjMWPGeKyt38zMVqxY4bEuNdTUmyi2EdOWjbqEPO6vLuOKy1h1OVU5LZ3TzyGmOGmbOP0exGWm\nr732WqWxpluYmbVt29ZjXQJnli6h1O9fbF1ciq2Mi0lbb/72t79NtulyfV1C/sYbbyTjdJljnKct\nWrTwWJc8ajqBWZpGGd9fvze69Lyc5mU+mjJ68cUXJ9v089e0jYceeigZ9/7773sc27Fq6o2mAUyd\nOjUZp8vGS3FexpQIvcbpZ6bXHLP0ehTbWk6bNs1jbe0e237r56nf+7hPuuRb54pZmh7bsmVLj2O6\njaaJxfcn3SM/Xb4d56Kmc7/zzjsejxo1Khn31ltveRzTh7t37+6xpvBNnz49GacpzeWQ2liofO2K\n+/fv77GeNzWd1yw9PsuWLfM43zlP01Dj+8+ZM8fjmJKFqtHjq63DTz755GScnrv1ujhixIhknKa7\n6jnTLL1/1TSNfKmN+vvGLP2NU4rXzOrS46hpOQcffHAyTq9PzzzzjMcxFUc/9/hbQ6/dmhKk89Is\nf0kPmPXu3Tt5rSmMHTp08Dj+DnziiSc81mMYP3+dHzrfzMz2228/j7X8Rr628PG3Sn39nmAlDgAA\nAAAAQAbwEAcAAAAAACADipJOpUvMdAlhz549k3G6TZeZmqUpEppCFbtCaSeAQlOa4hJvXTauXXA0\nLcAs7QASlxyXayVyXear6VNmZu3atfN4zZo1HmuqjVnaceyjjz7yOFYv1+Wp+t5m6bJ0TQuIS/F0\nmWS+lDv8i1bnv/rqqz3WJYlmZosWLfJ45MiRHsd0J13mGDvFDRw40OMDDjjA4zindPlr7GKmXT44\nxv+kn7N2bNCOGWZpus7DDz/s8YwZM5JxOu+185yZ2YABAzzW705Mp9LuEKW4NDx2M9DPQs9X+lma\npUu+FyxYkGzTtBdN740dcfTz1PeL6U756P5qB5XYXVD3P3by0PO+zsVyukZG2vVNu97o+c7M7LPP\nPvN4+PDhHo8fPz4Zp0vAtUuSWXqO1vscTVc3S1MbSaf6F+1qEpf/a4qvprlo+pRZel3UFIt47dM0\nDU2DM0vnnKZOxhRIVI2mrZ155pmV/u9maZrUo48+6nG8Lm699dYe77///sk2TQ/X1Mb4HnoPHH/v\naOpVKV4zq0uvQccee6zHsdvQCy+84LHel+o9o5lZp06dPD7ooIOSbZpWqb8v9N7JbMM0OaTzI6Ys\n6r2o/k7T+1Cz3B3g4j2Fnrtjyv/ee+/tsaYgz507Nxmn1+CGcjxZiQMAAAAAAJABPMQBAAAAAADI\nAB7iAAAAAAAAZECd1MSJNWY0f15be7Vv3z7nuMWLFyfbtAWmtlGMNS5ytTCN/7u+1txws7SVp8ax\nLZm2dI2tHTUnupTFz3WXXXbxeLfddku2ae6/1hPS9sRm6bHP9d0xS/PEY70izRnXPEb9u2Zmq1at\nMuSnx83M7MQTT/RY2zfGHFFtfTt69GiPY40FrQWw5557Jts0L13b0j/33HPJOK2Jo7VVKvt7SNtY\na+2NeAx1burnGI+Ttt3U74RZ7nz/CRMmJON0LpZi7aLYrlJr5Oh5TmOz9HP/6quvkm1aU0PfL37n\n9T11Psc6PVpnTNvBm6XnVH2PWPdG6w7Etrq6/7qP5VTPId81U+s3xJx+rd+gdb+0Bb1Zel086qij\nkm3aqlVrbcQ6ZToXy7leUdS8eXOPtQaOWTqXtHZJrK+hdai0dke8v9F6RlqTwyyteaXHkWtd1cTz\nn9Zv69Gjh8fx/n7KlCke6/1Lnz59knH6+6Ffv37JNq23qdfdWBNH655pPSWzDWuflat4TtX50rVr\nV4/j9VNrqOh9Sqyxqceuf//+yTa9rmt9z1hPpRTvaaoqHietKxbvKfW6M3HiRI/jtUprgul8jvcl\n+p2I9Vr1t6re28QahFrnqKHMPVbiAAAAAAAAZAAPcQAAAAAAADKgKC3GdVmTLtHWpalm6TKmuKxf\nt8X0J5WrtXdcNqntxrS1uVnahlOX1cXl8LqMKy6ZLZfl4fo5mqVLF+Px1c9El6lpSoBZuqxR24rr\nsnOzdOmqpoeYpUtcdVljvuWo+drtFtquvlToZxGXCJ9yyike69x+6aWXknHaVlxbn8YWynrszjvv\nvGRbr169PNaUyr///e/JOE2Zi8dH/1vKdUmrziOzdCm3bsuXirb77rt7HFuu6lLYmNqoS9FffPFF\nj3WZePxbcdltKcy5+N+Q61oV/9v1XBbnjh5H/Xdx2bjSa2k8R+txzZeiqulZMZ1H06liS1dNM9Hz\nfpyXpXC8c4nH8JBDDvFYP/+YZqxLyvW6pdfc+H7x3K1zXc/XmlZgll6rc6Wom5X2cTLb8L9d7yPj\nf7t+Znq905a1ZunSff3ea0tdszSdKqY26v2TtoOP96h67Yv7W+rHLhc9pvH8pOlPOk7vL8zS85/e\no+i9q1ma2hhTdPTY6LXw7bffTsbp3IxlGvLNzXISf9/pcdXPLLZo19+S+e5v9BjH9tSffPKJx3pO\njb8J813jc40rNTHFSdN74zZ9BjBv3ryc76llFvT3aLx/0VTJI488Mtmm12Qt5/Hmm28m42J6VUPA\nShwAAAAAAIAM4CEOAAAAAABABtRJOlVMS4lL3f5fviVlcQnqHnvs4bEuGY3L43Qpmi5Dj0u+mzVr\n5rEuoTRLO61od40VK1Yk43SJZdxWLmkbusTeLF2eGDsl6GeiKWy6bNjMbM2aNR7rMuLYzUyXkccU\nO03h0OXGMc1AUwv0+xL3v9Q7qcRl2D179vT4uOOOS7ZpV46FCxd6rJXbzdJjolXoYzrBoEGDPI6d\njXRJ5aRJkzzWY2qWnnPicdTvXTmlxen5NXYY0yXG77zzjsdx2b4uce3YsaPHseuQLl+On7++/9ix\nYz2OneF0f+M1JNcxzJK435oSs3z5co9jKrHOl27duiXb9Lyk3VTyLeXWVICtt94659+Kx1ivoRrH\n/y5NKdClzmbp90mPf0Pp9FBX9L87ps1omrCm+8YuNTondOl/XDau3VPiuVZTUrU7nKb/mKXXg3if\npte/UpiXVaHfU02nNzNbtmyZx/q59+3bNxmn6VT6OcfURk2/iedvvd/U4xE72+m2eD+mr8vlftUs\nTSmM5QD03lPnX5wDet+rxzr+btF7pfi3NG3jf//3fz2eOnVqMi6WG1DlMOcKEY+Pzs1XX33V49hR\nTueVHsd4P6y/F+Nc1PS3fN2L9D3jcSuX+RefBehnFDvA6fHQ3w/x2Oix1w5/sUyKbovpcno8dN5r\n11uzdC42lLnHShwAAAAAAIAM4CEOAAAAAABABvAQBwAAAAAAIAPqpCZOzE/UvDfNG47tnjWfNLb+\nO/DAAz3Wdm8xl1vzfGPunNL88lgHQlse63vovpul9UDifmguckPJnast+tnFY6252kuXLk22aT6q\n1rfRXEWz9PPK97di/rfSv63t6WLepb5/rMNR6vVTND811lU44IADPI6t3fW7rXUtYpv3ww47zGPN\nI87XcjXWNtKWfpqrGmsb6Xcj5v6X4rGrqphz/fHHH3s8evRoj2OusJ7/tA7A3nvvnYzTmh+xTtnw\n4cM9/uijjzyOOeN6nEqx5XT8b9K5o9cSjc3SfPx4rdJacdpuXGs7mKVzVud9zO/XYxLnkdaSyBWb\npbnssRad/r1SPr/G3H/9TGKuvh4bbScc54fWpNL6GrHFuF5P432JtkzV1rjxu6nzPtaAK7VjVRV6\n3ZkyZUqyTa+TWlMutp3OVVMozkX9DsW5qN8TvefKd2xK8ZxaiHhfp69j/TGtu6d142K9Ij1Wek6O\nNcZ0XLxnefHFFz3W1tSxnbmey2OL8XI5ht8nnqPmzJnj8bPPPutxvvsb/T2hdVHN0uMf72/0/T//\n/HOP8823cpqLem8er2l6Hzpz5sxkm/5O0DpynTp1yvn+Gsf5lut3pVn6u/CFF17IuU96Hm4ox4yV\nOAAAAAAAABnAQxwAAAAAAIAMqJN0qrhUTJc16TI3bT0bxSWoupRfl73F5Vm69FCXSsbl5brMOC6t\n0ha5+t8Sl9HNnTvX45imU8rpVLpkLX6uH3zwgccxTUo/Zz2+cRmxfnb6ucbPUVM44vdAl4prW9WY\nwqffg7hkOdey51KhLYQ7dOiQbNPWwDEtUdMK9fjHeaTLjDUlJC7/1zSs2HZaW/xp+8aYqqctmvOl\n6ZTaXCxU/Ew0ZWfcuHEex3Q2fa0pVCeddFLOcdou1SxdNq6tr/PNt1IUj4Geiz788EOPJ0+enIzT\nzymmGWsqk86/2OY917kstiLX70VMX9VjrOfveG7U83lc/q+vS+146+cf06n0dfy8NDVGz635Upz0\nfiieu3VbTPkZO3asx7r0P6aVaCvVfNe+cjuf6v1InKd67erWrZvHMYVD54d+tvG6qCkE8T7r3Xff\n9Vi/P/E+VOdbKd7DVIeeT2OJhPfee89jTfWI10VNmzr00EM9ji2s9fwcvy8PP/ywx3r+j+dknZvl\nNt8KFe8ltMX4+PHjPY7pvfpbUufsz372s2ScplM9+eSTybYJEyZ4rL8n4vWtXOeffmfjdUZ/mz32\n2GPJNv0stdxDLKOh76nlFzTN1Mxs0KBBHsfvi871MWPGeKy/K8wa5jFkJQ4AAAAAAEAG8BAHAAAA\nAAAgA3iIAwAAAAAAkAFFqYmjebqzZ8/O+e80ny224dSaGvly7jXHVXOUY7sxFdvl5srh0xxys7R1\ndcz1a4i5c7VF/9tiDrbm9sb6M2+//bbHmu+ttW3M0mOleZHaTtcsbTUXj6/WAtBaKjEHWutUxHzj\nUs8/1rkTcz91nsY2tVr7Id/3XOtyaMvG2CZZ5462uzYzu+OOOzzW+Rf3KZ4HkH5/Y3621rzQzz/m\nEet5eODAgR7H+g2ag37LLbck27Q+Vb75VuriMdDrk7ZejzWo9DqjbTfN0vonOt9ifSr921pfQ2vU\nmaXXYH3v+O923HFHj2OLca2lo98zs/R6UWrXSP0+x5x7/e/WOgBmaUthbR0e63DoXOzevbvHsX6g\nzuG//OUvyTZtn6rj8tURK2fxc9B5FM+VWvNE7z/ytQ7XY3zmmWcm43QOxxpwWudD77PifSjHMX87\n53zXRb2mxRpjffr08Xj//ff3ONY/0nP3sGHDkm2vv/56pX83njs4hlWnn2G+34H6u7Jv374e9+rV\nKxmn174HHngg2abHuJzvbwoR55ueQ1999dVk2/vvv++xXuNiXSM9pjqP+vXrl4w74YQTKh1nltZx\nnDFjhsdxLjZErMQBAAAAAADIAB7iAAAAAAAAZECdpFNFmuqgbbpj20Rt2a3Lis3S5Yy6XDsuA9Zl\nUjouLlHXJa5dunRJtukSKn1/XfZslr9dbikvpcu3bDxXKpRZ+vnpcuC43FjfX1s57rXXXsk4XVYX\n2zJq2z9NGYjLjcuZfmaacmaWfmZx7ui80nmpbcnN0vS3uE1pG/Frr7022abpeaU8p4otV9t1nW9m\nZv379/dYWzTGdJ0777zT44kTJybbSHWrnF5bNFUwLjnWbXEpsc4/TdOIx0fP03pejtc0XXreokWL\nnPuuKQSaWhX3MaZa6X6UcjpVvC/RZeNxSb8ux9f2qTvttFMyTtOptK14PNZPPfWUx48++miybfHi\nxTn3HzWj32c958Xzn14/9TsTSwjofJ4/f36yTVthx/to5Jfr2meWHkOdV+3atUvGnXzyyR5ramO8\nv7zxxhs91lRGM45bfYjXIz2PHn744R7H9DltOx3TfmJqDgqn8y+eJ/Vapc8N4vVOaXpqLL+h9zPx\nOvjggw96HNNkGzpW4gAAAAAAAGQAD3EAAAAAAAAyoCjpVLmWTOXr4JAvhUOXPMal50rTdOLyZu28\nka8DgS6Vi52NSBPYUKFLVXN9xmbpse7cubPHu+22WzJOl55/+umnyTZN6WDZauXyddrQ13o8zNJu\nYs2bN/c4pkzp8Wrfvr3H2q3IzOzmm2/2WNOnzEihKgZNXY3H8MQTT/S4devWHmv3FbM0nYqUxcLo\n+VDPUbELol534nVRlxYXeu7V6248N+pcj8dx1qxZHuv5IaYa6FJ0PT+YpSki8bxSSuLnr595vO/R\nz0TnYhyn51P9zGfOnJmMu/vuuz2O3yXUP/1utGzZ0uOePXsm43R+aPc6M45rXdHPXDsBHnHEEcm4\nI4880mNN4XjkkUeSccOHD/eY+9D6oddMPdeapWnBOv9it9bbb7/d4/g7EHUvX0c5pb8zDj744GSb\nzu2YEhc7RmYJK3EAAAAAAAAygIc4AAAAAAAAGcBDHAAAAAAAgAwoSk2cXGKLUX0da9hUR8wpz/W3\nYp6k5txpXYBYA6fUWqQWU75aJ1rLSGvidOrUKed7aEtssw3rrqD6Yh0OPT5aH6d3797JuAEDBnis\n+eAjR45Mxml+KnOqOHLVPtFjFl9r++nrrrsuGUeNhprJVyMsX+21XHVwqltLSr8X8e/q8V+5cmXO\n99A2rttss02yTetH5Ku7Vcq1sGKLVD32Wm+hb9++ybj99tvPY21T/thjjyXjJk2a5HEpf45Zlavm\nX6wtpfNv/PjxybbauD/GhucdbUOs8++UU05Jxmm9HK0V9pvf/CYZp3U+UT/0/Lrddtsl284444xK\n/81rr72WvH7llVc85pzasOjv9x//+Mcet2nTJhmndY7uuOOOZFuWf3ewEgcAAAAAACADeIgDAAAA\nAACQAfWaTlWfNCUkLm/WpaqaBhLTs/Q9SrldarE1a9bM4x49eni89dZbJ+M0DURbipvRzrGmdE5o\neoSZWZMmTTzWln4HHnhgMk7/nS7xHzFiRDKOY1V8uiS4Y8eOHsflxXqO0yXGo0ePzvl+qJn4WeZr\nqVmX4hJjnadLlizxOKZ2aPplbDGu6QqLFi3yuL7+G+tDvqXbbdu29ViXhpul7ahnzJjh8cMPP5yM\ni63h0bDodXGXXXbxWK+rZmarV6/2+PXXX0+2cb6tPr1Xj595165dPT7ttNM83nXXXZNxmiZ1yy23\neDx79uxa20/UDp1vxx57bLJN71813ebOO+9Mxmn6KhqWnXbayeODDjrI41gGYvLkyR6/++67db9j\nRcJKHAAAAAAAgAzgIQ4AAAAAAEAG8BAHAAAAAAAgA8qmJk6se7PZZpvlHKstXrUOQMxD1lzL+P6a\nd0v+cn5ad8MsbQO42267eRxzHDUvecqUKcm2LLeMawjy1cTROhe77767x7FFqrZ51za4c+fOTcYx\nP4pPW8MPGTLEY60JYJbWLfn973/vsdZrQOnQ61Y8h2pdgPnz53sc2+jqe8Q6ZlrvbPPNN/c41nEp\np3OC3ovsu+++Hu+9997JuKVLl3o8atQoj+fNm1eHe4fatuWWW3rcu3dvj2NdxVWrVnn82Wef1f2O\nlaj4uep5J96zHH300R7vueeeHsd71HHjxnn80EMPecx9Z8Oj9YyOOOKIZJteZ7SuZmwxjoYj/h4Z\nOHCgxx06dPA43pfoNbOU5ikrcQAAAAAAADKAhzgAAAAAAAAZUDbpVHFJpS7Jiu3jdNlyXJKV6z1I\np6q+mNqmS1xbt27tcWxlu3jxYo91eT+qLs4PTV2Lyxc1RUJb4sYl3++//77H48eP93jt2rU121lU\nWTy+ulRcW8PHY6PLiidMmFBHe4eGKF861YcffuhxbKur6SKxdfgmm2zisaYoxO9nKYv/rboEXNOp\nYvrw9OnTPX7ppZc8jtdFNCzxeDdt2tRjvb+J5169p9EUf3w//cz1nGOWpuv369cv2abp+1tttZXH\nMWXx9ttv91jT3tAwaNru4MGDPd5+++2TcStXrvR49OjRHtNSvGHR+dymTZtk24ABAzzW35ILFixI\nxr388st1tHf1i5U4AAAAAAAAGcBDHAAAAAAAgAwo23QqXeatXXTMzP7xj394rEtc45JKlrhWnx6P\nmIqmy+XWrVvncUzXee+99zxesmRJbe9iWdMUwJgOqMdk1qxZHs+cOTMZ99Zbb3msSxtLqTJ8VrRo\n0SJ5vddee3msS4/jufDxxx/3mCXG5UXnuVmaWqydPFq1apWM23bbbT3WtCszs2XLlnmsHanK6Zyg\nneHMzNq3b++xztM4F5999lmPP/30U49J127Y8nU1zdXxzczsnXfe8ZiUuarR+0vtRmWW3l9269Yt\n2abpGHq/OXbs2GScphkz/+pfvhTVXr16eRznkaYC6/1qOV2PskBTIuOc7dixo8f6m/yDDz5Ixn3x\nxRd1tHf1i5U4AAAAAAAAGcBDHAAAAAAAgAzgIQ4AAAAAAEAGlE1NnJjjqLn5b775ZrJt0aJFHmtN\nHG1pHd8j1g8gT7ZwMZ9VayVMmzbN4/gZa9tqPRZmfP41pTWjtBaGWVqrYeHChR7HminaejO2GkZx\nadtns3R+6DzSOgxmZuPGjfM4zj+Unny1AHKdl2M9Mv2u6b8xS6+her4op/O11kQxS2vkTJ061WOt\ne2OWzkXaGmeXzgGtORZrS02YMMHjOI9QuFhzsXHjf/3sWbFiRbLt/fff91hr4jzyyCPJuPjvUL82\n3njj5HXTpk091nvUNWvWJOP0t9/HH39cR3uHmtJaVbFNvN5HaI0jvV6apb/lSwkrcQAAAAAAADKA\nhzgAAAAAAAAZ0Kgqy5gbNWpUMmueNYUnLsXTdmYx1Ufp8qy6TjWoqKjIvSNV0FCOoX6uulTOzGyb\nbbbxWFuuxuM0d+5cj5cvX55sa6DL89+pqKjoUxtvVMzjmG8OqAb6mde6LM7FOHdatmxZ6bg4j0q4\ntbHtkNcAAAF1SURBVG0m52JDoecETU+I22IaZW2nVWZxLsb0Dl36r9tiemqpLge3Mp6Lel6O5+is\nHe+GNBf1HBTnm97fb7XVVsk2PZetXLnS49WrV9d0l7KiJOainlM11hbUZmlaaind6zSkuVgbNOW4\nS5cuybbWrVt7rPM0psfNnz+/jvauzhQ0F1mJAwAAAAAAkAE8xAEAAAAAAMgAHuIAAAAAAABkQNnW\nxMmaUstxLFMlkW9c7piLJYG5WAKYiyWBuVgCmIslgblYApiLJYGaOAAAAAAAAKWChzgAAAAAAAAZ\n0Pj7hySWmNmcutgR5NW+Ft+LY1h/OI7ZxzEsDRzH7OMYlgaOY/ZxDEsDxzH7OIaloaDjWKWaOAAA\nAAAAAKgfpFMBAAAAAABkAA9xAAAAAAAAMoCHOAAAAAAAABnAQxwAAAAAAIAM4CEOAAAAAABABvAQ\nBwAAAAAAIAN4iAMAAAAAAJABPMQBAAAAAADIAB7iAAAAAAAAZMD/AfGMaVx7bQ+aAAAAAElFTkSu\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26780dc5be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "n = 10  # how many digits we will display\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_train[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    predicted = model.predict(x_train_vec[i:i+1]).reshape((28,28))\n",
    "    plt.imshow(predicted)\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一部はっきりしないweakな４もあるが、単層のときと比べて、かなり４と９の区別がつきやすくなった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "自己符号化器はPCAと関係があるが、PCAは線形な分離しかできない。最近は非線形な<a href=\"http://cs.stanford.edu/people/karpathy/tsnejs/\">tSNE</a>が注目されている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
