{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データを０～９のすべてに拡大してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 300\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_train_idx = np.logical_or(y_train > -1, False)\n",
    "keep_test_idx = np.logical_or(y_test > -1, False)\n",
    "\n",
    "x_train = x_train[keep_train_idx]\n",
    "x_test = x_test[keep_test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_vec = x_train.reshape(x_train.shape[0], 784)\n",
    "x_test_vec = x_test.reshape(x_test.shape[0], 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中央の要素数は5個に設定する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 505       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               600       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 784)               79184     \n",
      "=================================================================\n",
      "Total params: 158,789\n",
      "Trainable params: 158,789\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, activation='sigmoid', input_shape=(784,)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(5, activation='sigmoid'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(100, activation='sigmoid'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(784, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0707 - acc: 0.0095 - val_loss: 0.0630 - val_acc: 0.0118\n",
      "Epoch 2/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0626 - acc: 0.0105 - val_loss: 0.0609 - val_acc: 0.0129\n",
      "Epoch 3/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0615 - acc: 0.0107 - val_loss: 0.0590 - val_acc: 0.0100\n",
      "Epoch 4/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0597 - acc: 0.0125 - val_loss: 0.0563 - val_acc: 0.0135\n",
      "Epoch 5/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0588 - acc: 0.0120 - val_loss: 0.0554 - val_acc: 0.0172\n",
      "Epoch 6/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0584 - acc: 0.0128 - val_loss: 0.0549 - val_acc: 0.0155\n",
      "Epoch 7/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0579 - acc: 0.0144 - val_loss: 0.0539 - val_acc: 0.0147\n",
      "Epoch 8/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0573 - acc: 0.0142 - val_loss: 0.0530 - val_acc: 0.0160\n",
      "Epoch 9/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0569 - acc: 0.0149 - val_loss: 0.0523 - val_acc: 0.0150\n",
      "Epoch 10/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0565 - acc: 0.0146 - val_loss: 0.0518 - val_acc: 0.0157\n",
      "Epoch 11/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0562 - acc: 0.0143 - val_loss: 0.0514 - val_acc: 0.0149\n",
      "Epoch 12/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0560 - acc: 0.0142 - val_loss: 0.0511 - val_acc: 0.0158\n",
      "Epoch 13/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0557 - acc: 0.0144 - val_loss: 0.0506 - val_acc: 0.0125\n",
      "Epoch 14/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0556 - acc: 0.0135 - val_loss: 0.0504 - val_acc: 0.0152\n",
      "Epoch 15/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0554 - acc: 0.0129 - val_loss: 0.0501 - val_acc: 0.0139\n",
      "Epoch 16/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0551 - acc: 0.0121 - val_loss: 0.0495 - val_acc: 0.0181\n",
      "Epoch 17/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0549 - acc: 0.0116 - val_loss: 0.0494 - val_acc: 0.0124\n",
      "Epoch 18/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0548 - acc: 0.0116 - val_loss: 0.0490 - val_acc: 0.0129\n",
      "Epoch 19/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0545 - acc: 0.0107 - val_loss: 0.0487 - val_acc: 0.0145\n",
      "Epoch 20/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0543 - acc: 0.0109 - val_loss: 0.0483 - val_acc: 0.0131\n",
      "Epoch 21/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0541 - acc: 0.0103 - val_loss: 0.0479 - val_acc: 0.0131\n",
      "Epoch 22/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0539 - acc: 0.0101 - val_loss: 0.0476 - val_acc: 0.0116\n",
      "Epoch 23/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0537 - acc: 0.0102 - val_loss: 0.0474 - val_acc: 0.0126\n",
      "Epoch 24/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0536 - acc: 0.0097 - val_loss: 0.0471 - val_acc: 0.0141\n",
      "Epoch 25/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0534 - acc: 0.0091 - val_loss: 0.0471 - val_acc: 0.0133\n",
      "Epoch 26/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0532 - acc: 0.0099 - val_loss: 0.0466 - val_acc: 0.0114\n",
      "Epoch 27/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0532 - acc: 0.0090 - val_loss: 0.0467 - val_acc: 0.0118\n",
      "Epoch 28/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0530 - acc: 0.0093 - val_loss: 0.0466 - val_acc: 0.0116\n",
      "Epoch 29/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0529 - acc: 0.0089 - val_loss: 0.0465 - val_acc: 0.0124\n",
      "Epoch 30/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0527 - acc: 0.0094 - val_loss: 0.0462 - val_acc: 0.0123\n",
      "Epoch 31/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0527 - acc: 0.0090 - val_loss: 0.0462 - val_acc: 0.0114\n",
      "Epoch 32/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0526 - acc: 0.0089 - val_loss: 0.0461 - val_acc: 0.0106\n",
      "Epoch 33/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0525 - acc: 0.0091 - val_loss: 0.0461 - val_acc: 0.0105\n",
      "Epoch 34/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0525 - acc: 0.0088 - val_loss: 0.0460 - val_acc: 0.0101\n",
      "Epoch 35/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0523 - acc: 0.0094 - val_loss: 0.0459 - val_acc: 0.0095\n",
      "Epoch 36/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0523 - acc: 0.0098 - val_loss: 0.0460 - val_acc: 0.0123\n",
      "Epoch 37/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0521 - acc: 0.0094 - val_loss: 0.0458 - val_acc: 0.0112\n",
      "Epoch 38/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0522 - acc: 0.0095 - val_loss: 0.0458 - val_acc: 0.0106\n",
      "Epoch 39/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0521 - acc: 0.0094 - val_loss: 0.0457 - val_acc: 0.0106\n",
      "Epoch 40/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0520 - acc: 0.0089 - val_loss: 0.0458 - val_acc: 0.0102\n",
      "Epoch 41/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0519 - acc: 0.0092 - val_loss: 0.0456 - val_acc: 0.0118\n",
      "Epoch 42/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0518 - acc: 0.0091 - val_loss: 0.0456 - val_acc: 0.0112\n",
      "Epoch 43/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0518 - acc: 0.0094 - val_loss: 0.0456 - val_acc: 0.0103\n",
      "Epoch 44/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0517 - acc: 0.0090 - val_loss: 0.0457 - val_acc: 0.0105\n",
      "Epoch 45/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0516 - acc: 0.0084 - val_loss: 0.0456 - val_acc: 0.0105\n",
      "Epoch 46/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0517 - acc: 0.0089 - val_loss: 0.0456 - val_acc: 0.0107\n",
      "Epoch 47/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0516 - acc: 0.0088 - val_loss: 0.0456 - val_acc: 0.0113\n",
      "Epoch 48/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0515 - acc: 0.0088 - val_loss: 0.0456 - val_acc: 0.0108\n",
      "Epoch 49/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0514 - acc: 0.0086 - val_loss: 0.0455 - val_acc: 0.0114\n",
      "Epoch 50/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0513 - acc: 0.0089 - val_loss: 0.0455 - val_acc: 0.0101\n",
      "Epoch 51/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0513 - acc: 0.0087 - val_loss: 0.0455 - val_acc: 0.0094\n",
      "Epoch 52/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0513 - acc: 0.0089 - val_loss: 0.0454 - val_acc: 0.0103\n",
      "Epoch 53/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0513 - acc: 0.0091 - val_loss: 0.0456 - val_acc: 0.0109\n",
      "Epoch 54/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0512 - acc: 0.0088 - val_loss: 0.0456 - val_acc: 0.0105\n",
      "Epoch 55/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0511 - acc: 0.0098 - val_loss: 0.0456 - val_acc: 0.0105\n",
      "Epoch 56/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0511 - acc: 0.0093 - val_loss: 0.0455 - val_acc: 0.0110\n",
      "Epoch 57/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0510 - acc: 0.0090 - val_loss: 0.0456 - val_acc: 0.0100\n",
      "Epoch 58/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0510 - acc: 0.0092 - val_loss: 0.0456 - val_acc: 0.0114\n",
      "Epoch 59/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0509 - acc: 0.0091 - val_loss: 0.0456 - val_acc: 0.0107\n",
      "Epoch 60/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0509 - acc: 0.0095 - val_loss: 0.0456 - val_acc: 0.0110\n",
      "Epoch 61/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0509 - acc: 0.0088 - val_loss: 0.0456 - val_acc: 0.0103\n",
      "Epoch 62/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0508 - acc: 0.0089 - val_loss: 0.0457 - val_acc: 0.0106\n",
      "Epoch 63/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0508 - acc: 0.0092 - val_loss: 0.0458 - val_acc: 0.0109\n",
      "Epoch 64/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 4s - loss: 0.0507 - acc: 0.0094 - val_loss: 0.0458 - val_acc: 0.0117\n",
      "Epoch 65/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0508 - acc: 0.0092 - val_loss: 0.0457 - val_acc: 0.0107\n",
      "Epoch 66/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0507 - acc: 0.0092 - val_loss: 0.0458 - val_acc: 0.0104\n",
      "Epoch 67/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0507 - acc: 0.0092 - val_loss: 0.0458 - val_acc: 0.0110\n",
      "Epoch 68/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0505 - acc: 0.0091 - val_loss: 0.0458 - val_acc: 0.0106\n",
      "Epoch 69/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0506 - acc: 0.0088 - val_loss: 0.0459 - val_acc: 0.0108\n",
      "Epoch 70/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0505 - acc: 0.0091 - val_loss: 0.0458 - val_acc: 0.0104\n",
      "Epoch 71/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0505 - acc: 0.0093 - val_loss: 0.0458 - val_acc: 0.0118\n",
      "Epoch 72/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0505 - acc: 0.0091 - val_loss: 0.0460 - val_acc: 0.0114\n",
      "Epoch 73/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0505 - acc: 0.0094 - val_loss: 0.0460 - val_acc: 0.0108\n",
      "Epoch 74/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0504 - acc: 0.0095 - val_loss: 0.0461 - val_acc: 0.0108\n",
      "Epoch 75/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0504 - acc: 0.0100 - val_loss: 0.0459 - val_acc: 0.0107\n",
      "Epoch 76/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0504 - acc: 0.0091 - val_loss: 0.0460 - val_acc: 0.0114\n",
      "Epoch 77/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0502 - acc: 0.0088 - val_loss: 0.0462 - val_acc: 0.0103\n",
      "Epoch 78/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0502 - acc: 0.0096 - val_loss: 0.0461 - val_acc: 0.0117\n",
      "Epoch 79/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0503 - acc: 0.0095 - val_loss: 0.0459 - val_acc: 0.0110\n",
      "Epoch 80/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0503 - acc: 0.0091 - val_loss: 0.0461 - val_acc: 0.0115\n",
      "Epoch 81/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0502 - acc: 0.0092 - val_loss: 0.0459 - val_acc: 0.0126\n",
      "Epoch 82/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0502 - acc: 0.0100 - val_loss: 0.0461 - val_acc: 0.0119\n",
      "Epoch 83/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0501 - acc: 0.0084 - val_loss: 0.0460 - val_acc: 0.0118\n",
      "Epoch 84/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0501 - acc: 0.0089 - val_loss: 0.0461 - val_acc: 0.0110\n",
      "Epoch 85/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0501 - acc: 0.0090 - val_loss: 0.0461 - val_acc: 0.0104\n",
      "Epoch 86/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0500 - acc: 0.0092 - val_loss: 0.0462 - val_acc: 0.0107\n",
      "Epoch 87/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0502 - acc: 0.0091 - val_loss: 0.0461 - val_acc: 0.0117\n",
      "Epoch 88/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0500 - acc: 0.0090 - val_loss: 0.0462 - val_acc: 0.0111\n",
      "Epoch 89/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0500 - acc: 0.0092 - val_loss: 0.0464 - val_acc: 0.0117\n",
      "Epoch 90/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0500 - acc: 0.0095 - val_loss: 0.0463 - val_acc: 0.0119\n",
      "Epoch 91/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0500 - acc: 0.0094 - val_loss: 0.0464 - val_acc: 0.0126\n",
      "Epoch 92/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0499 - acc: 0.0089 - val_loss: 0.0464 - val_acc: 0.0127\n",
      "Epoch 93/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0500 - acc: 0.0088 - val_loss: 0.0463 - val_acc: 0.0124\n",
      "Epoch 94/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0499 - acc: 0.0094 - val_loss: 0.0465 - val_acc: 0.0113\n",
      "Epoch 95/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0498 - acc: 0.0094 - val_loss: 0.0463 - val_acc: 0.0113\n",
      "Epoch 96/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0498 - acc: 0.0092 - val_loss: 0.0464 - val_acc: 0.0131\n",
      "Epoch 97/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0498 - acc: 0.0092 - val_loss: 0.0464 - val_acc: 0.0114\n",
      "Epoch 98/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0499 - acc: 0.0089 - val_loss: 0.0464 - val_acc: 0.0118\n",
      "Epoch 99/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0498 - acc: 0.0096 - val_loss: 0.0463 - val_acc: 0.0124\n",
      "Epoch 100/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0498 - acc: 0.0094 - val_loss: 0.0465 - val_acc: 0.0113\n",
      "Epoch 101/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0498 - acc: 0.0094 - val_loss: 0.0465 - val_acc: 0.0113\n",
      "Epoch 102/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0498 - acc: 0.0096 - val_loss: 0.0465 - val_acc: 0.0126\n",
      "Epoch 103/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0497 - acc: 0.0092 - val_loss: 0.0467 - val_acc: 0.0112\n",
      "Epoch 104/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0497 - acc: 0.0100 - val_loss: 0.0465 - val_acc: 0.0121\n",
      "Epoch 105/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0496 - acc: 0.0103 - val_loss: 0.0468 - val_acc: 0.0111\n",
      "Epoch 106/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0497 - acc: 0.0091 - val_loss: 0.0463 - val_acc: 0.0125\n",
      "Epoch 107/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0496 - acc: 0.0094 - val_loss: 0.0466 - val_acc: 0.0118\n",
      "Epoch 108/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0496 - acc: 0.0102 - val_loss: 0.0466 - val_acc: 0.0122\n",
      "Epoch 109/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0496 - acc: 0.0094 - val_loss: 0.0468 - val_acc: 0.0114\n",
      "Epoch 110/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0496 - acc: 0.0099 - val_loss: 0.0466 - val_acc: 0.0117\n",
      "Epoch 111/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0496 - acc: 0.0100 - val_loss: 0.0464 - val_acc: 0.0116\n",
      "Epoch 112/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0496 - acc: 0.0096 - val_loss: 0.0468 - val_acc: 0.0119\n",
      "Epoch 113/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0495 - acc: 0.0097 - val_loss: 0.0467 - val_acc: 0.0115\n",
      "Epoch 114/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0496 - acc: 0.0089 - val_loss: 0.0467 - val_acc: 0.0110\n",
      "Epoch 115/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0494 - acc: 0.0097 - val_loss: 0.0467 - val_acc: 0.0124\n",
      "Epoch 116/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0495 - acc: 0.0092 - val_loss: 0.0466 - val_acc: 0.0129\n",
      "Epoch 117/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0495 - acc: 0.0096 - val_loss: 0.0469 - val_acc: 0.0122\n",
      "Epoch 118/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0494 - acc: 0.0097 - val_loss: 0.0469 - val_acc: 0.0120\n",
      "Epoch 119/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0495 - acc: 0.0096 - val_loss: 0.0467 - val_acc: 0.0116\n",
      "Epoch 120/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0495 - acc: 0.0093 - val_loss: 0.0467 - val_acc: 0.0116\n",
      "Epoch 121/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0495 - acc: 0.0097 - val_loss: 0.0467 - val_acc: 0.0114\n",
      "Epoch 122/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0495 - acc: 0.0088 - val_loss: 0.0470 - val_acc: 0.0115\n",
      "Epoch 123/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0494 - acc: 0.0098 - val_loss: 0.0469 - val_acc: 0.0120\n",
      "Epoch 124/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0495 - acc: 0.0095 - val_loss: 0.0466 - val_acc: 0.0121\n",
      "Epoch 125/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0494 - acc: 0.0098 - val_loss: 0.0469 - val_acc: 0.0110\n",
      "Epoch 126/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0493 - acc: 0.0098 - val_loss: 0.0471 - val_acc: 0.0119\n",
      "Epoch 127/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 4s - loss: 0.0494 - acc: 0.0098 - val_loss: 0.0470 - val_acc: 0.0117\n",
      "Epoch 128/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0493 - acc: 0.0100 - val_loss: 0.0469 - val_acc: 0.0123\n",
      "Epoch 129/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0493 - acc: 0.0099 - val_loss: 0.0471 - val_acc: 0.0116\n",
      "Epoch 130/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0493 - acc: 0.0099 - val_loss: 0.0470 - val_acc: 0.0117\n",
      "Epoch 131/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0494 - acc: 0.0098 - val_loss: 0.0468 - val_acc: 0.0121\n",
      "Epoch 132/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0493 - acc: 0.0095 - val_loss: 0.0470 - val_acc: 0.0115\n",
      "Epoch 133/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0493 - acc: 0.0099 - val_loss: 0.0470 - val_acc: 0.0122\n",
      "Epoch 134/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0493 - acc: 0.0099 - val_loss: 0.0470 - val_acc: 0.0115\n",
      "Epoch 135/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0493 - acc: 0.0102 - val_loss: 0.0467 - val_acc: 0.0126\n",
      "Epoch 136/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0491 - acc: 0.0101 - val_loss: 0.0473 - val_acc: 0.0119\n",
      "Epoch 137/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0493 - acc: 0.0099 - val_loss: 0.0471 - val_acc: 0.0108\n",
      "Epoch 138/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0492 - acc: 0.0093 - val_loss: 0.0472 - val_acc: 0.0119\n",
      "Epoch 139/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0492 - acc: 0.0096 - val_loss: 0.0471 - val_acc: 0.0115\n",
      "Epoch 140/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0492 - acc: 0.0097 - val_loss: 0.0474 - val_acc: 0.0114\n",
      "Epoch 141/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0492 - acc: 0.0087 - val_loss: 0.0472 - val_acc: 0.0118\n",
      "Epoch 142/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0492 - acc: 0.0101 - val_loss: 0.0473 - val_acc: 0.0117\n",
      "Epoch 143/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0493 - acc: 0.0096 - val_loss: 0.0474 - val_acc: 0.0120\n",
      "Epoch 144/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0492 - acc: 0.0092 - val_loss: 0.0472 - val_acc: 0.0124\n",
      "Epoch 145/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0492 - acc: 0.0098 - val_loss: 0.0472 - val_acc: 0.0103\n",
      "Epoch 146/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0492 - acc: 0.0098 - val_loss: 0.0473 - val_acc: 0.0122\n",
      "Epoch 147/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0491 - acc: 0.0106 - val_loss: 0.0475 - val_acc: 0.0111\n",
      "Epoch 148/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0492 - acc: 0.0098 - val_loss: 0.0472 - val_acc: 0.0116\n",
      "Epoch 149/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0492 - acc: 0.0093 - val_loss: 0.0474 - val_acc: 0.0111\n",
      "Epoch 150/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0490 - acc: 0.0100 - val_loss: 0.0472 - val_acc: 0.0121\n",
      "Epoch 151/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0492 - acc: 0.0098 - val_loss: 0.0473 - val_acc: 0.0121\n",
      "Epoch 152/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0491 - acc: 0.0101 - val_loss: 0.0474 - val_acc: 0.0118\n",
      "Epoch 153/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0491 - acc: 0.0098 - val_loss: 0.0475 - val_acc: 0.0115\n",
      "Epoch 154/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0491 - acc: 0.0104 - val_loss: 0.0476 - val_acc: 0.0112\n",
      "Epoch 155/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0490 - acc: 0.0095 - val_loss: 0.0476 - val_acc: 0.0119\n",
      "Epoch 156/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0491 - acc: 0.0103 - val_loss: 0.0477 - val_acc: 0.0110\n",
      "Epoch 157/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0491 - acc: 0.0099 - val_loss: 0.0476 - val_acc: 0.0117\n",
      "Epoch 158/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0491 - acc: 0.0103 - val_loss: 0.0475 - val_acc: 0.0119\n",
      "Epoch 159/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0492 - acc: 0.0096 - val_loss: 0.0476 - val_acc: 0.0120\n",
      "Epoch 160/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0491 - acc: 0.0096 - val_loss: 0.0476 - val_acc: 0.0124\n",
      "Epoch 161/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0490 - acc: 0.0103 - val_loss: 0.0475 - val_acc: 0.0126\n",
      "Epoch 162/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0490 - acc: 0.0105 - val_loss: 0.0476 - val_acc: 0.0127\n",
      "Epoch 163/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0490 - acc: 0.0102 - val_loss: 0.0476 - val_acc: 0.0126\n",
      "Epoch 164/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0490 - acc: 0.0103 - val_loss: 0.0477 - val_acc: 0.0120\n",
      "Epoch 165/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0490 - acc: 0.0103 - val_loss: 0.0476 - val_acc: 0.0126\n",
      "Epoch 166/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0490 - acc: 0.0102 - val_loss: 0.0477 - val_acc: 0.0124\n",
      "Epoch 167/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0490 - acc: 0.0102 - val_loss: 0.0477 - val_acc: 0.0118\n",
      "Epoch 168/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0489 - acc: 0.0104 - val_loss: 0.0477 - val_acc: 0.0111\n",
      "Epoch 169/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0490 - acc: 0.0103 - val_loss: 0.0478 - val_acc: 0.0111\n",
      "Epoch 170/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0490 - acc: 0.0105 - val_loss: 0.0478 - val_acc: 0.0118\n",
      "Epoch 171/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0490 - acc: 0.0101 - val_loss: 0.0478 - val_acc: 0.0112\n",
      "Epoch 172/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0489 - acc: 0.0102 - val_loss: 0.0477 - val_acc: 0.0131\n",
      "Epoch 173/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0490 - acc: 0.0107 - val_loss: 0.0476 - val_acc: 0.0117\n",
      "Epoch 174/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0489 - acc: 0.0109 - val_loss: 0.0478 - val_acc: 0.0119\n",
      "Epoch 175/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0489 - acc: 0.0105 - val_loss: 0.0478 - val_acc: 0.0126\n",
      "Epoch 176/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0490 - acc: 0.0100 - val_loss: 0.0479 - val_acc: 0.0108\n",
      "Epoch 177/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0489 - acc: 0.0104 - val_loss: 0.0478 - val_acc: 0.0121\n",
      "Epoch 178/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0489 - acc: 0.0106 - val_loss: 0.0480 - val_acc: 0.0111\n",
      "Epoch 179/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0488 - acc: 0.0106 - val_loss: 0.0477 - val_acc: 0.0114\n",
      "Epoch 180/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0490 - acc: 0.0103 - val_loss: 0.0475 - val_acc: 0.0119\n",
      "Epoch 181/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0489 - acc: 0.0106 - val_loss: 0.0477 - val_acc: 0.0117\n",
      "Epoch 182/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0489 - acc: 0.0103 - val_loss: 0.0480 - val_acc: 0.0123\n",
      "Epoch 183/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0488 - acc: 0.0103 - val_loss: 0.0482 - val_acc: 0.0118\n",
      "Epoch 184/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0488 - acc: 0.0093 - val_loss: 0.0482 - val_acc: 0.0134\n",
      "Epoch 185/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0488 - acc: 0.0111 - val_loss: 0.0483 - val_acc: 0.0104\n",
      "Epoch 186/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0489 - acc: 0.0103 - val_loss: 0.0480 - val_acc: 0.0114\n",
      "Epoch 187/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0489 - acc: 0.0104 - val_loss: 0.0481 - val_acc: 0.0126\n",
      "Epoch 188/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0488 - acc: 0.0105 - val_loss: 0.0479 - val_acc: 0.0113\n",
      "Epoch 189/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0488 - acc: 0.0101 - val_loss: 0.0480 - val_acc: 0.0121\n",
      "Epoch 190/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 4s - loss: 0.0489 - acc: 0.0106 - val_loss: 0.0478 - val_acc: 0.0125\n",
      "Epoch 191/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0488 - acc: 0.0110 - val_loss: 0.0482 - val_acc: 0.0117\n",
      "Epoch 192/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0488 - acc: 0.0106 - val_loss: 0.0480 - val_acc: 0.0117\n",
      "Epoch 193/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0488 - acc: 0.0099 - val_loss: 0.0482 - val_acc: 0.0115\n",
      "Epoch 194/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0487 - acc: 0.0108 - val_loss: 0.0480 - val_acc: 0.0115\n",
      "Epoch 195/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0488 - acc: 0.0106 - val_loss: 0.0481 - val_acc: 0.0121\n",
      "Epoch 196/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0488 - acc: 0.0106 - val_loss: 0.0483 - val_acc: 0.0132\n",
      "Epoch 197/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0489 - acc: 0.0109 - val_loss: 0.0482 - val_acc: 0.0121\n",
      "Epoch 198/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0487 - acc: 0.0102 - val_loss: 0.0481 - val_acc: 0.0114\n",
      "Epoch 199/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0488 - acc: 0.0107 - val_loss: 0.0484 - val_acc: 0.0117\n",
      "Epoch 200/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0488 - acc: 0.0106 - val_loss: 0.0482 - val_acc: 0.0130\n",
      "Epoch 201/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0487 - acc: 0.0106 - val_loss: 0.0483 - val_acc: 0.0107\n",
      "Epoch 202/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0487 - acc: 0.0109 - val_loss: 0.0482 - val_acc: 0.0122\n",
      "Epoch 203/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0487 - acc: 0.0106 - val_loss: 0.0482 - val_acc: 0.0122\n",
      "Epoch 204/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0487 - acc: 0.0102 - val_loss: 0.0482 - val_acc: 0.0118\n",
      "Epoch 205/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0488 - acc: 0.0105 - val_loss: 0.0484 - val_acc: 0.0118\n",
      "Epoch 206/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0488 - acc: 0.0113 - val_loss: 0.0484 - val_acc: 0.0122\n",
      "Epoch 207/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0488 - acc: 0.0104 - val_loss: 0.0482 - val_acc: 0.0124\n",
      "Epoch 208/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0487 - acc: 0.0108 - val_loss: 0.0484 - val_acc: 0.0112\n",
      "Epoch 209/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0488 - acc: 0.0101 - val_loss: 0.0484 - val_acc: 0.0120\n",
      "Epoch 210/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0487 - acc: 0.0107 - val_loss: 0.0483 - val_acc: 0.0114\n",
      "Epoch 211/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0487 - acc: 0.0108 - val_loss: 0.0483 - val_acc: 0.0113\n",
      "Epoch 212/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0487 - acc: 0.0107 - val_loss: 0.0484 - val_acc: 0.0119\n",
      "Epoch 213/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0487 - acc: 0.0106 - val_loss: 0.0481 - val_acc: 0.0118\n",
      "Epoch 214/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0486 - acc: 0.0106 - val_loss: 0.0486 - val_acc: 0.0119\n",
      "Epoch 215/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0487 - acc: 0.0105 - val_loss: 0.0485 - val_acc: 0.0122\n",
      "Epoch 216/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0486 - acc: 0.0109 - val_loss: 0.0483 - val_acc: 0.0130\n",
      "Epoch 217/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0487 - acc: 0.0103 - val_loss: 0.0486 - val_acc: 0.0123\n",
      "Epoch 218/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0487 - acc: 0.0112 - val_loss: 0.0485 - val_acc: 0.0121\n",
      "Epoch 219/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0487 - acc: 0.0111 - val_loss: 0.0483 - val_acc: 0.0115\n",
      "Epoch 220/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0487 - acc: 0.0101 - val_loss: 0.0487 - val_acc: 0.0113\n",
      "Epoch 221/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0487 - acc: 0.0108 - val_loss: 0.0484 - val_acc: 0.0132\n",
      "Epoch 222/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0486 - acc: 0.0113 - val_loss: 0.0488 - val_acc: 0.0113\n",
      "Epoch 223/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0487 - acc: 0.0112 - val_loss: 0.0486 - val_acc: 0.0128\n",
      "Epoch 224/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0486 - acc: 0.0111 - val_loss: 0.0485 - val_acc: 0.0112\n",
      "Epoch 225/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0487 - acc: 0.0110 - val_loss: 0.0487 - val_acc: 0.0125\n",
      "Epoch 226/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0487 - acc: 0.0114 - val_loss: 0.0487 - val_acc: 0.0112\n",
      "Epoch 227/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0487 - acc: 0.0115 - val_loss: 0.0489 - val_acc: 0.0125\n",
      "Epoch 228/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0486 - acc: 0.0107 - val_loss: 0.0489 - val_acc: 0.0115\n",
      "Epoch 229/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0486 - acc: 0.0106 - val_loss: 0.0486 - val_acc: 0.0115\n",
      "Epoch 230/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0486 - acc: 0.0110 - val_loss: 0.0489 - val_acc: 0.0122\n",
      "Epoch 231/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0485 - acc: 0.0106 - val_loss: 0.0488 - val_acc: 0.0123\n",
      "Epoch 232/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0486 - acc: 0.0108 - val_loss: 0.0488 - val_acc: 0.0128\n",
      "Epoch 233/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0486 - acc: 0.0115 - val_loss: 0.0487 - val_acc: 0.0126\n",
      "Epoch 234/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0486 - acc: 0.0117 - val_loss: 0.0486 - val_acc: 0.0112\n",
      "Epoch 235/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0485 - acc: 0.0118 - val_loss: 0.0487 - val_acc: 0.0114\n",
      "Epoch 236/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0486 - acc: 0.0113 - val_loss: 0.0489 - val_acc: 0.0127\n",
      "Epoch 237/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0485 - acc: 0.0110 - val_loss: 0.0490 - val_acc: 0.0127\n",
      "Epoch 238/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0487 - acc: 0.0107 - val_loss: 0.0489 - val_acc: 0.0128\n",
      "Epoch 239/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0485 - acc: 0.0106 - val_loss: 0.0485 - val_acc: 0.0131\n",
      "Epoch 240/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0486 - acc: 0.0115 - val_loss: 0.0487 - val_acc: 0.0133\n",
      "Epoch 241/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0485 - acc: 0.0116 - val_loss: 0.0487 - val_acc: 0.0120\n",
      "Epoch 242/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0486 - acc: 0.0116 - val_loss: 0.0487 - val_acc: 0.0129\n",
      "Epoch 243/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0486 - acc: 0.0113 - val_loss: 0.0489 - val_acc: 0.0124\n",
      "Epoch 244/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0486 - acc: 0.0115 - val_loss: 0.0491 - val_acc: 0.0132\n",
      "Epoch 245/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0485 - acc: 0.0113 - val_loss: 0.0487 - val_acc: 0.0125\n",
      "Epoch 246/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0486 - acc: 0.0115 - val_loss: 0.0488 - val_acc: 0.0127\n",
      "Epoch 247/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0486 - acc: 0.0118 - val_loss: 0.0491 - val_acc: 0.0128\n",
      "Epoch 248/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0486 - acc: 0.0108 - val_loss: 0.0487 - val_acc: 0.0131\n",
      "Epoch 249/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0485 - acc: 0.0110 - val_loss: 0.0489 - val_acc: 0.0115\n",
      "Epoch 250/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0486 - acc: 0.0113 - val_loss: 0.0490 - val_acc: 0.0134\n",
      "Epoch 251/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0486 - acc: 0.0113 - val_loss: 0.0491 - val_acc: 0.0126\n",
      "Epoch 252/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0485 - acc: 0.0114 - val_loss: 0.0489 - val_acc: 0.0115\n",
      "Epoch 253/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 5s - loss: 0.0486 - acc: 0.0116 - val_loss: 0.0490 - val_acc: 0.0118\n",
      "Epoch 254/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0485 - acc: 0.0107 - val_loss: 0.0493 - val_acc: 0.0130\n",
      "Epoch 255/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0485 - acc: 0.0111 - val_loss: 0.0492 - val_acc: 0.0120\n",
      "Epoch 256/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0485 - acc: 0.0113 - val_loss: 0.0489 - val_acc: 0.0125\n",
      "Epoch 257/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0485 - acc: 0.0119 - val_loss: 0.0489 - val_acc: 0.0133\n",
      "Epoch 258/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0485 - acc: 0.0119 - val_loss: 0.0490 - val_acc: 0.0131\n",
      "Epoch 259/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0485 - acc: 0.0118 - val_loss: 0.0485 - val_acc: 0.0134\n",
      "Epoch 260/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0485 - acc: 0.0109 - val_loss: 0.0489 - val_acc: 0.0121\n",
      "Epoch 261/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0485 - acc: 0.0114 - val_loss: 0.0490 - val_acc: 0.0125\n",
      "Epoch 262/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0485 - acc: 0.0113 - val_loss: 0.0493 - val_acc: 0.0122\n",
      "Epoch 263/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0485 - acc: 0.0116 - val_loss: 0.0491 - val_acc: 0.0126\n",
      "Epoch 264/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0484 - acc: 0.0116 - val_loss: 0.0489 - val_acc: 0.0120\n",
      "Epoch 265/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0485 - acc: 0.0122 - val_loss: 0.0491 - val_acc: 0.0130\n",
      "Epoch 266/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0485 - acc: 0.0114 - val_loss: 0.0492 - val_acc: 0.0128\n",
      "Epoch 267/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0484 - acc: 0.0106 - val_loss: 0.0490 - val_acc: 0.0132\n",
      "Epoch 268/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0485 - acc: 0.0115 - val_loss: 0.0492 - val_acc: 0.0128\n",
      "Epoch 269/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0484 - acc: 0.0110 - val_loss: 0.0491 - val_acc: 0.01220.01\n",
      "Epoch 270/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0484 - acc: 0.0115 - val_loss: 0.0488 - val_acc: 0.0128\n",
      "Epoch 271/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0485 - acc: 0.0111 - val_loss: 0.0490 - val_acc: 0.0112\n",
      "Epoch 272/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0485 - acc: 0.0112 - val_loss: 0.0492 - val_acc: 0.0121\n",
      "Epoch 273/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0485 - acc: 0.0113 - val_loss: 0.0491 - val_acc: 0.0130\n",
      "Epoch 274/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0485 - acc: 0.0117 - val_loss: 0.0489 - val_acc: 0.0130\n",
      "Epoch 275/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0485 - acc: 0.0121 - val_loss: 0.0491 - val_acc: 0.0124\n",
      "Epoch 276/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0484 - acc: 0.0117 - val_loss: 0.0493 - val_acc: 0.0127\n",
      "Epoch 277/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0483 - acc: 0.0121 - val_loss: 0.0490 - val_acc: 0.0132\n",
      "Epoch 278/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0485 - acc: 0.0125 - val_loss: 0.0489 - val_acc: 0.0129\n",
      "Epoch 279/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0484 - acc: 0.0122 - val_loss: 0.0495 - val_acc: 0.0129\n",
      "Epoch 280/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0484 - acc: 0.0119 - val_loss: 0.0494 - val_acc: 0.0128\n",
      "Epoch 281/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0485 - acc: 0.0124 - val_loss: 0.0493 - val_acc: 0.0139\n",
      "Epoch 282/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0485 - acc: 0.0119 - val_loss: 0.0490 - val_acc: 0.0129\n",
      "Epoch 283/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0484 - acc: 0.0116 - val_loss: 0.0491 - val_acc: 0.0114\n",
      "Epoch 284/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0484 - acc: 0.0121 - val_loss: 0.0490 - val_acc: 0.0124\n",
      "Epoch 285/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0485 - acc: 0.0119 - val_loss: 0.0492 - val_acc: 0.0115\n",
      "Epoch 286/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0484 - acc: 0.0118 - val_loss: 0.0492 - val_acc: 0.0132\n",
      "Epoch 287/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0484 - acc: 0.0122 - val_loss: 0.0493 - val_acc: 0.0124\n",
      "Epoch 288/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0484 - acc: 0.0115 - val_loss: 0.0495 - val_acc: 0.0126\n",
      "Epoch 289/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0484 - acc: 0.0116 - val_loss: 0.0497 - val_acc: 0.0134\n",
      "Epoch 290/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0484 - acc: 0.0115 - val_loss: 0.0492 - val_acc: 0.0120\n",
      "Epoch 291/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0484 - acc: 0.0117 - val_loss: 0.0494 - val_acc: 0.0122\n",
      "Epoch 292/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0484 - acc: 0.0115 - val_loss: 0.0493 - val_acc: 0.0120\n",
      "Epoch 293/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0484 - acc: 0.0118 - val_loss: 0.0493 - val_acc: 0.0131\n",
      "Epoch 294/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0483 - acc: 0.0123 - val_loss: 0.0490 - val_acc: 0.0126\n",
      "Epoch 295/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0484 - acc: 0.0116 - val_loss: 0.0492 - val_acc: 0.0128\n",
      "Epoch 296/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0485 - acc: 0.0112 - val_loss: 0.0494 - val_acc: 0.0119\n",
      "Epoch 297/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0484 - acc: 0.0120 - val_loss: 0.0492 - val_acc: 0.0135\n",
      "Epoch 298/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0484 - acc: 0.0116 - val_loss: 0.0495 - val_acc: 0.0127\n",
      "Epoch 299/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0484 - acc: 0.0119 - val_loss: 0.0494 - val_acc: 0.0129\n",
      "Epoch 300/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0484 - acc: 0.0117 - val_loss: 0.0496 - val_acc: 0.0120\n",
      "Test loss: 0.0496380061507\n",
      "Test accuracy: 0.012\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train_vec, x_train_vec,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test_vec, x_test_vec))\n",
    "score = model.evaluate(x_test_vec, x_test_vec, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データが増えるのでもう少し時間がかかる。ちなみにmnistには各数字が同数ずつ入っているわけではない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHEAAADqCAYAAAAlBtnSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFAAvcQAAAAAAAArg/wE8l+/BMgnaPgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18893ee3080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "n = 10  # how many digits we will display\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_train[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    predicted = model.predict(x_train_vec[i:i+1]).reshape((28,28))\n",
    "    plt.imshow(predicted)\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "０、３，５，８のような全体的に丸いグループと”１”とは判別できる。やはり一番強い特徴は全体的な「傾き」と「大きさ」なのかもしれない。４については９ぽくなったり、丸くなったりしている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
