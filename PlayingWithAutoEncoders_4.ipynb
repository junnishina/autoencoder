{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中間層の要素数は、独立な特徴量の数のようなものに対応している。我々が求めるautoencoderは、４と９を分離可能な最小の特徴量を持つものである。中間層の要素数が1個でははっきり分離してくれなかった。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 200\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_train_idx = np.logical_or(y_train == 4, y_train == 9)\n",
    "keep_test_idx = np.logical_or(y_test ==4, y_test == 9)\n",
    "\n",
    "x_train = x_train[keep_train_idx]\n",
    "x_test = x_test[keep_test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_vec = x_train.reshape(x_train.shape[0], 784)\n",
    "x_test_vec = x_test.reshape(x_test.shape[0], 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは中間層の要素数を12個に設定する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 12)                9420      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 784)               10192     \n",
      "=================================================================\n",
      "Total params: 19,612\n",
      "Trainable params: 19,612\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, activation='sigmoid', input_shape=(784,)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(784, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11791 samples, validate on 1991 samples\n",
      "Epoch 1/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.1396 - acc: 0.0056 - val_loss: 0.0846 - val_acc: 0.0045\n",
      "Epoch 2/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0703 - acc: 0.0072 - val_loss: 0.0572 - val_acc: 0.0095\n",
      "Epoch 3/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0564 - acc: 0.0081 - val_loss: 0.0527 - val_acc: 0.0095\n",
      "Epoch 4/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0538 - acc: 0.0079 - val_loss: 0.0522 - val_acc: 0.0095\n",
      "Epoch 5/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0532 - acc: 0.0092 - val_loss: 0.0521 - val_acc: 0.0095\n",
      "Epoch 6/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0530 - acc: 0.0106 - val_loss: 0.0521 - val_acc: 0.0095\n",
      "Epoch 7/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0530 - acc: 0.0092 - val_loss: 0.0521 - val_acc: 0.0095\n",
      "Epoch 8/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0530 - acc: 0.0095 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 9/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0530 - acc: 0.0099 - val_loss: 0.0521 - val_acc: 0.0095\n",
      "Epoch 10/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0530 - acc: 0.0101 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 11/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0530 - acc: 0.0100 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 12/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0529 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 13/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0529 - acc: 0.0100 - val_loss: 0.0521 - val_acc: 0.0095\n",
      "Epoch 14/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0529 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 15/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0529 - acc: 0.0102 - val_loss: 0.0521 - val_acc: 0.0095\n",
      "Epoch 16/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0529 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 17/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0529 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 18/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0529 - acc: 0.0103 - val_loss: 0.0521 - val_acc: 0.0095\n",
      "Epoch 19/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0529 - acc: 0.0102 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 20/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0529 - acc: 0.0101 - val_loss: 0.0521 - val_acc: 0.0095\n",
      "Epoch 21/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0529 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 22/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0529 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 23/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0529 - acc: 0.0103 - val_loss: 0.0521 - val_acc: 0.0095\n",
      "Epoch 24/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0529 - acc: 0.0103 - val_loss: 0.0521 - val_acc: 0.0095\n",
      "Epoch 25/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0529 - acc: 0.0103 - val_loss: 0.0521 - val_acc: 0.0095\n",
      "Epoch 26/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0528 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 27/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0528 - acc: 0.0103 - val_loss: 0.0521 - val_acc: 0.0095\n",
      "Epoch 28/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0528 - acc: 0.0102 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 29/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0528 - acc: 0.0103 - val_loss: 0.0520 - val_acc: 0.0095\n",
      "Epoch 30/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0527 - acc: 0.0103 - val_loss: 0.0518 - val_acc: 0.0095\n",
      "Epoch 31/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0523 - acc: 0.0102 - val_loss: 0.0513 - val_acc: 0.0060\n",
      "Epoch 32/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0519 - acc: 0.0089 - val_loss: 0.0508 - val_acc: 0.0095\n",
      "Epoch 33/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0512 - acc: 0.0078 - val_loss: 0.0500 - val_acc: 0.0060\n",
      "Epoch 34/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0506 - acc: 0.0089 - val_loss: 0.0493 - val_acc: 0.0045\n",
      "Epoch 35/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0500 - acc: 0.0095 - val_loss: 0.0486 - val_acc: 0.0070\n",
      "Epoch 36/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0494 - acc: 0.0105 - val_loss: 0.0479 - val_acc: 0.0080\n",
      "Epoch 37/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0489 - acc: 0.0092 - val_loss: 0.0472 - val_acc: 0.0085\n",
      "Epoch 38/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0484 - acc: 0.0107 - val_loss: 0.0465 - val_acc: 0.0080\n",
      "Epoch 39/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0477 - acc: 0.0103 - val_loss: 0.0456 - val_acc: 0.0080\n",
      "Epoch 40/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0472 - acc: 0.0098 - val_loss: 0.0449 - val_acc: 0.0116\n",
      "Epoch 41/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0466 - acc: 0.0094 - val_loss: 0.0442 - val_acc: 0.0095\n",
      "Epoch 42/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0463 - acc: 0.0105 - val_loss: 0.0437 - val_acc: 0.0110\n",
      "Epoch 43/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0459 - acc: 0.0100 - val_loss: 0.0433 - val_acc: 0.0116\n",
      "Epoch 44/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0456 - acc: 0.0098 - val_loss: 0.0428 - val_acc: 0.0105\n",
      "Epoch 45/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0453 - acc: 0.0093 - val_loss: 0.0424 - val_acc: 0.0105\n",
      "Epoch 46/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0452 - acc: 0.0105 - val_loss: 0.0421 - val_acc: 0.0110\n",
      "Epoch 47/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0449 - acc: 0.0107 - val_loss: 0.0418 - val_acc: 0.0105\n",
      "Epoch 48/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0447 - acc: 0.0097 - val_loss: 0.0416 - val_acc: 0.0121\n",
      "Epoch 49/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0446 - acc: 0.0097 - val_loss: 0.0413 - val_acc: 0.0126\n",
      "Epoch 50/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0444 - acc: 0.0115 - val_loss: 0.0410 - val_acc: 0.0121\n",
      "Epoch 51/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0444 - acc: 0.0106 - val_loss: 0.0409 - val_acc: 0.0116\n",
      "Epoch 52/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0442 - acc: 0.0109 - val_loss: 0.0407 - val_acc: 0.0121\n",
      "Epoch 53/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0442 - acc: 0.0110 - val_loss: 0.0405 - val_acc: 0.0121\n",
      "Epoch 54/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0440 - acc: 0.0103 - val_loss: 0.0403 - val_acc: 0.0121\n",
      "Epoch 55/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0441 - acc: 0.0107 - val_loss: 0.0403 - val_acc: 0.0121\n",
      "Epoch 56/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0439 - acc: 0.0111 - val_loss: 0.0401 - val_acc: 0.0121\n",
      "Epoch 57/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0438 - acc: 0.0111 - val_loss: 0.0400 - val_acc: 0.0121\n",
      "Epoch 58/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0437 - acc: 0.0102 - val_loss: 0.0400 - val_acc: 0.0121\n",
      "Epoch 59/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0438 - acc: 0.0114 - val_loss: 0.0398 - val_acc: 0.0116\n",
      "Epoch 60/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0437 - acc: 0.0098 - val_loss: 0.0398 - val_acc: 0.0110\n",
      "Epoch 61/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0437 - acc: 0.0096 - val_loss: 0.0397 - val_acc: 0.0110\n",
      "Epoch 62/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0437 - acc: 0.0111 - val_loss: 0.0397 - val_acc: 0.0100\n",
      "Epoch 63/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0435 - acc: 0.0088 - val_loss: 0.0397 - val_acc: 0.0100\n",
      "Epoch 64/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11791/11791 [==============================] - 0s - loss: 0.0434 - acc: 0.0108 - val_loss: 0.0394 - val_acc: 0.0110\n",
      "Epoch 65/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0435 - acc: 0.0101 - val_loss: 0.0395 - val_acc: 0.0100\n",
      "Epoch 66/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0435 - acc: 0.0103 - val_loss: 0.0394 - val_acc: 0.0100\n",
      "Epoch 67/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0434 - acc: 0.0109 - val_loss: 0.0393 - val_acc: 0.0100\n",
      "Epoch 68/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0434 - acc: 0.0106 - val_loss: 0.0392 - val_acc: 0.0100\n",
      "Epoch 69/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0434 - acc: 0.0092 - val_loss: 0.0392 - val_acc: 0.0110\n",
      "Epoch 70/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0434 - acc: 0.0095 - val_loss: 0.0392 - val_acc: 0.0100\n",
      "Epoch 71/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0432 - acc: 0.0095 - val_loss: 0.0391 - val_acc: 0.0100\n",
      "Epoch 72/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0432 - acc: 0.0105 - val_loss: 0.0391 - val_acc: 0.0095\n",
      "Epoch 73/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0433 - acc: 0.0108 - val_loss: 0.0390 - val_acc: 0.0095\n",
      "Epoch 74/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0431 - acc: 0.0103 - val_loss: 0.0389 - val_acc: 0.0090\n",
      "Epoch 75/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0432 - acc: 0.0103 - val_loss: 0.0388 - val_acc: 0.0095\n",
      "Epoch 76/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0432 - acc: 0.0089 - val_loss: 0.0388 - val_acc: 0.0095\n",
      "Epoch 77/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0431 - acc: 0.0089 - val_loss: 0.0387 - val_acc: 0.0095\n",
      "Epoch 78/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0431 - acc: 0.0109 - val_loss: 0.0387 - val_acc: 0.0095\n",
      "Epoch 79/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0429 - acc: 0.0088 - val_loss: 0.0387 - val_acc: 0.0095\n",
      "Epoch 80/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0430 - acc: 0.0096 - val_loss: 0.0387 - val_acc: 0.0095\n",
      "Epoch 81/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0430 - acc: 0.0095 - val_loss: 0.0386 - val_acc: 0.0095\n",
      "Epoch 82/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0427 - acc: 0.0094 - val_loss: 0.0384 - val_acc: 0.0095\n",
      "Epoch 83/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0426 - acc: 0.0102 - val_loss: 0.0383 - val_acc: 0.0095\n",
      "Epoch 84/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0424 - acc: 0.0109 - val_loss: 0.0381 - val_acc: 0.0090\n",
      "Epoch 85/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0424 - acc: 0.0100 - val_loss: 0.0381 - val_acc: 0.0090\n",
      "Epoch 86/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0423 - acc: 0.0100 - val_loss: 0.0377 - val_acc: 0.0090\n",
      "Epoch 87/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0423 - acc: 0.0102 - val_loss: 0.0376 - val_acc: 0.0085\n",
      "Epoch 88/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0421 - acc: 0.0094 - val_loss: 0.0375 - val_acc: 0.0085\n",
      "Epoch 89/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0420 - acc: 0.0106 - val_loss: 0.0374 - val_acc: 0.0085\n",
      "Epoch 90/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0420 - acc: 0.0099 - val_loss: 0.0375 - val_acc: 0.0095\n",
      "Epoch 91/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0420 - acc: 0.0103 - val_loss: 0.0374 - val_acc: 0.0090\n",
      "Epoch 92/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0418 - acc: 0.0102 - val_loss: 0.0372 - val_acc: 0.0100\n",
      "Epoch 93/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0418 - acc: 0.0105 - val_loss: 0.0371 - val_acc: 0.0095\n",
      "Epoch 94/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0417 - acc: 0.0106 - val_loss: 0.0371 - val_acc: 0.0090\n",
      "Epoch 95/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0418 - acc: 0.0100 - val_loss: 0.0371 - val_acc: 0.0090\n",
      "Epoch 96/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0417 - acc: 0.0101 - val_loss: 0.0370 - val_acc: 0.0095\n",
      "Epoch 97/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0419 - acc: 0.0093 - val_loss: 0.0371 - val_acc: 0.0100\n",
      "Epoch 98/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0416 - acc: 0.0103 - val_loss: 0.0368 - val_acc: 0.0105\n",
      "Epoch 99/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0415 - acc: 0.0103 - val_loss: 0.0369 - val_acc: 0.0090\n",
      "Epoch 100/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0417 - acc: 0.0104 - val_loss: 0.0368 - val_acc: 0.0095\n",
      "Epoch 101/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0417 - acc: 0.0106 - val_loss: 0.0370 - val_acc: 0.0110\n",
      "Epoch 102/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0417 - acc: 0.0096 - val_loss: 0.0368 - val_acc: 0.0100\n",
      "Epoch 103/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0415 - acc: 0.0100 - val_loss: 0.0368 - val_acc: 0.0105\n",
      "Epoch 104/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0415 - acc: 0.0085 - val_loss: 0.0367 - val_acc: 0.0100\n",
      "Epoch 105/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0414 - acc: 0.0092 - val_loss: 0.0368 - val_acc: 0.0110\n",
      "Epoch 106/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0414 - acc: 0.0096 - val_loss: 0.0366 - val_acc: 0.0105\n",
      "Epoch 107/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0413 - acc: 0.0098 - val_loss: 0.0365 - val_acc: 0.0110\n",
      "Epoch 108/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0412 - acc: 0.0103 - val_loss: 0.0364 - val_acc: 0.0110\n",
      "Epoch 109/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0409 - acc: 0.0095 - val_loss: 0.0361 - val_acc: 0.0105\n",
      "Epoch 110/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0410 - acc: 0.0104 - val_loss: 0.0361 - val_acc: 0.0100\n",
      "Epoch 111/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0409 - acc: 0.0088 - val_loss: 0.0359 - val_acc: 0.0100\n",
      "Epoch 112/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0408 - acc: 0.0090 - val_loss: 0.0358 - val_acc: 0.0100\n",
      "Epoch 113/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0406 - acc: 0.0098 - val_loss: 0.0358 - val_acc: 0.0100\n",
      "Epoch 114/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0406 - acc: 0.0086 - val_loss: 0.0356 - val_acc: 0.0100\n",
      "Epoch 115/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0405 - acc: 0.0094 - val_loss: 0.0357 - val_acc: 0.0090\n",
      "Epoch 116/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0405 - acc: 0.0082 - val_loss: 0.0356 - val_acc: 0.0090\n",
      "Epoch 117/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0406 - acc: 0.0080 - val_loss: 0.0355 - val_acc: 0.0105\n",
      "Epoch 118/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0406 - acc: 0.0076 - val_loss: 0.0354 - val_acc: 0.0090\n",
      "Epoch 119/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0403 - acc: 0.0091 - val_loss: 0.0352 - val_acc: 0.0095\n",
      "Epoch 120/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0403 - acc: 0.0075 - val_loss: 0.0353 - val_acc: 0.0116\n",
      "Epoch 121/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0404 - acc: 0.0086 - val_loss: 0.0354 - val_acc: 0.0100\n",
      "Epoch 122/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0404 - acc: 0.0085 - val_loss: 0.0351 - val_acc: 0.0100\n",
      "Epoch 123/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0403 - acc: 0.0083 - val_loss: 0.0351 - val_acc: 0.0100\n",
      "Epoch 124/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0401 - acc: 0.0087 - val_loss: 0.0351 - val_acc: 0.0100\n",
      "Epoch 125/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0403 - acc: 0.0080 - val_loss: 0.0351 - val_acc: 0.0100\n",
      "Epoch 126/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0403 - acc: 0.0084 - val_loss: 0.0351 - val_acc: 0.0095\n",
      "Epoch 127/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11791/11791 [==============================] - 0s - loss: 0.0402 - acc: 0.0074 - val_loss: 0.0352 - val_acc: 0.0105\n",
      "Epoch 128/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0403 - acc: 0.0081 - val_loss: 0.0350 - val_acc: 0.0116\n",
      "Epoch 129/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0402 - acc: 0.0084 - val_loss: 0.0350 - val_acc: 0.0116\n",
      "Epoch 130/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0401 - acc: 0.0081 - val_loss: 0.0351 - val_acc: 0.0110\n",
      "Epoch 131/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0402 - acc: 0.0085 - val_loss: 0.0351 - val_acc: 0.0105\n",
      "Epoch 132/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0400 - acc: 0.0081 - val_loss: 0.0348 - val_acc: 0.0121\n",
      "Epoch 133/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0401 - acc: 0.0067 - val_loss: 0.0350 - val_acc: 0.0090\n",
      "Epoch 134/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0401 - acc: 0.0072 - val_loss: 0.0350 - val_acc: 0.0105\n",
      "Epoch 135/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0401 - acc: 0.0090 - val_loss: 0.0348 - val_acc: 0.0110\n",
      "Epoch 136/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0402 - acc: 0.0085 - val_loss: 0.0348 - val_acc: 0.0100\n",
      "Epoch 137/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0400 - acc: 0.0074 - val_loss: 0.0351 - val_acc: 0.0116\n",
      "Epoch 138/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0400 - acc: 0.0080 - val_loss: 0.0347 - val_acc: 0.0110\n",
      "Epoch 139/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0400 - acc: 0.0084 - val_loss: 0.0347 - val_acc: 0.0100\n",
      "Epoch 140/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0400 - acc: 0.0098 - val_loss: 0.0348 - val_acc: 0.0095\n",
      "Epoch 141/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0400 - acc: 0.0085 - val_loss: 0.0349 - val_acc: 0.0110\n",
      "Epoch 142/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0400 - acc: 0.0081 - val_loss: 0.0347 - val_acc: 0.0105\n",
      "Epoch 143/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0401 - acc: 0.0093 - val_loss: 0.0348 - val_acc: 0.0095\n",
      "Epoch 144/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0400 - acc: 0.0081 - val_loss: 0.0346 - val_acc: 0.0095\n",
      "Epoch 145/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0399 - acc: 0.0080 - val_loss: 0.0348 - val_acc: 0.0100\n",
      "Epoch 146/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0399 - acc: 0.0086 - val_loss: 0.0346 - val_acc: 0.0095\n",
      "Epoch 147/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0400 - acc: 0.0092 - val_loss: 0.0348 - val_acc: 0.0105\n",
      "Epoch 148/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0399 - acc: 0.0092 - val_loss: 0.0348 - val_acc: 0.0095\n",
      "Epoch 149/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0398 - acc: 0.0086 - val_loss: 0.0347 - val_acc: 0.0100\n",
      "Epoch 150/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0400 - acc: 0.0085 - val_loss: 0.0347 - val_acc: 0.0100\n",
      "Epoch 151/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0399 - acc: 0.0081 - val_loss: 0.0348 - val_acc: 0.0095\n",
      "Epoch 152/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0398 - acc: 0.0097 - val_loss: 0.0345 - val_acc: 0.0090\n",
      "Epoch 153/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0398 - acc: 0.0081 - val_loss: 0.0345 - val_acc: 0.0105\n",
      "Epoch 154/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0399 - acc: 0.0071 - val_loss: 0.0345 - val_acc: 0.0100\n",
      "Epoch 155/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0398 - acc: 0.0085 - val_loss: 0.0346 - val_acc: 0.0100\n",
      "Epoch 156/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0398 - acc: 0.0081 - val_loss: 0.0347 - val_acc: 0.0100\n",
      "Epoch 157/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0399 - acc: 0.0087 - val_loss: 0.0346 - val_acc: 0.0095\n",
      "Epoch 158/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0399 - acc: 0.0088 - val_loss: 0.0344 - val_acc: 0.0095\n",
      "Epoch 159/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0399 - acc: 0.0082 - val_loss: 0.0344 - val_acc: 0.0105\n",
      "Epoch 160/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0397 - acc: 0.0088 - val_loss: 0.0344 - val_acc: 0.0100\n",
      "Epoch 161/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0400 - acc: 0.0077 - val_loss: 0.0346 - val_acc: 0.0095\n",
      "Epoch 162/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0398 - acc: 0.0079 - val_loss: 0.0344 - val_acc: 0.0090\n",
      "Epoch 163/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0399 - acc: 0.0076 - val_loss: 0.0344 - val_acc: 0.0095\n",
      "Epoch 164/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0396 - acc: 0.0080 - val_loss: 0.0344 - val_acc: 0.0095\n",
      "Epoch 165/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0397 - acc: 0.0087 - val_loss: 0.0344 - val_acc: 0.0095\n",
      "Epoch 166/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0398 - acc: 0.0087 - val_loss: 0.0345 - val_acc: 0.0095\n",
      "Epoch 167/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0399 - acc: 0.0076 - val_loss: 0.0346 - val_acc: 0.0095\n",
      "Epoch 168/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0397 - acc: 0.0079 - val_loss: 0.0345 - val_acc: 0.0095\n",
      "Epoch 169/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0398 - acc: 0.0083 - val_loss: 0.0344 - val_acc: 0.0095\n",
      "Epoch 170/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0399 - acc: 0.0086 - val_loss: 0.0345 - val_acc: 0.0095\n",
      "Epoch 171/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0397 - acc: 0.0090 - val_loss: 0.0345 - val_acc: 0.0095\n",
      "Epoch 172/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0397 - acc: 0.0082 - val_loss: 0.0345 - val_acc: 0.0095\n",
      "Epoch 173/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0397 - acc: 0.0091 - val_loss: 0.0343 - val_acc: 0.0095\n",
      "Epoch 174/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0398 - acc: 0.0075 - val_loss: 0.0343 - val_acc: 0.0090\n",
      "Epoch 175/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0397 - acc: 0.0087 - val_loss: 0.0343 - val_acc: 0.0090\n",
      "Epoch 176/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0399 - acc: 0.0083 - val_loss: 0.0345 - val_acc: 0.0090\n",
      "Epoch 177/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0398 - acc: 0.0083 - val_loss: 0.0345 - val_acc: 0.0090\n",
      "Epoch 178/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0396 - acc: 0.0083 - val_loss: 0.0344 - val_acc: 0.0095\n",
      "Epoch 179/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0396 - acc: 0.0087 - val_loss: 0.0343 - val_acc: 0.0095\n",
      "Epoch 180/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0398 - acc: 0.0081 - val_loss: 0.0344 - val_acc: 0.0095\n",
      "Epoch 181/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0396 - acc: 0.0078 - val_loss: 0.0344 - val_acc: 0.0095\n",
      "Epoch 182/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0398 - acc: 0.0085 - val_loss: 0.0343 - val_acc: 0.0100\n",
      "Epoch 183/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0397 - acc: 0.0087 - val_loss: 0.0342 - val_acc: 0.0095\n",
      "Epoch 184/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0398 - acc: 0.0081 - val_loss: 0.0343 - val_acc: 0.0090\n",
      "Epoch 185/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0397 - acc: 0.0084 - val_loss: 0.0344 - val_acc: 0.0100\n",
      "Epoch 186/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0396 - acc: 0.0083 - val_loss: 0.0342 - val_acc: 0.0090\n",
      "Epoch 187/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0397 - acc: 0.0082 - val_loss: 0.0345 - val_acc: 0.0090\n",
      "Epoch 188/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0395 - acc: 0.0081 - val_loss: 0.0342 - val_acc: 0.0095\n",
      "Epoch 189/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0397 - acc: 0.0083 - val_loss: 0.0343 - val_acc: 0.0090\n",
      "Epoch 190/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11791/11791 [==============================] - 0s - loss: 0.0397 - acc: 0.0081 - val_loss: 0.0343 - val_acc: 0.0095\n",
      "Epoch 191/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0397 - acc: 0.0082 - val_loss: 0.0343 - val_acc: 0.0090\n",
      "Epoch 192/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0396 - acc: 0.0084 - val_loss: 0.0342 - val_acc: 0.0095\n",
      "Epoch 193/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0397 - acc: 0.0092 - val_loss: 0.0343 - val_acc: 0.0095\n",
      "Epoch 194/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0397 - acc: 0.0085 - val_loss: 0.0344 - val_acc: 0.0095\n",
      "Epoch 195/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0395 - acc: 0.0092 - val_loss: 0.0343 - val_acc: 0.0095\n",
      "Epoch 196/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0396 - acc: 0.0080 - val_loss: 0.0342 - val_acc: 0.0095\n",
      "Epoch 197/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0395 - acc: 0.0082 - val_loss: 0.0343 - val_acc: 0.0095\n",
      "Epoch 198/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0396 - acc: 0.0078 - val_loss: 0.0342 - val_acc: 0.0095\n",
      "Epoch 199/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0395 - acc: 0.0090 - val_loss: 0.0343 - val_acc: 0.0095\n",
      "Epoch 200/200\n",
      "11791/11791 [==============================] - 0s - loss: 0.0396 - acc: 0.0085 - val_loss: 0.0343 - val_acc: 0.0095\n",
      "Test loss: 0.0343062102707\n",
      "Test accuracy: 0.0095429432446\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train_vec, x_train_vec,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test_vec, x_test_vec))\n",
    "score = model.evaluate(x_test_vec, x_test_vec, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中間要素数がきわどくなると、ロスがなかなか落ちにくくなるが、今日はこれくらいのエポック数で勘弁してやる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHEAAADqCAYAAAAlBtnSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKID/BxegoIw2PwRUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x271a94d75c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "n = 10  # how many digits we will display\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_train[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    predicted = model.predict(x_train_vec[i:i+1]).reshape((28,28))\n",
    "    plt.imshow(predicted)\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大局的な形状を優先的に再現してくれるものの、右上の部分が開いているかどうかというディテールについては、あまり区別してくれないため、出力を見て４か９かを判断するのは厳しい。このやり方では、要素数のみによる最適化はこのへんが限界なのか。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
